[
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "SQL",
    "section": "",
    "text": "Here is a simple query that selects the first five rows (and all columns, based on the * wildcard) from the questions table.\nselect * from questions limit 5\nTo run this from R we provide the SQL syntax as a string as the second argument to dbGetQuery.\n\nlibrary(RSQLite)\ndrv &lt;- dbDriver(\"SQLite\")\ndir &lt;- 'data' # relative or absolute path to where the .db file is\ndbFilename &lt;- 'stackoverflow-2021.db'\ndb &lt;- dbConnect(drv, dbname = file.path(dir, dbFilename))\ndbGetQuery(db, \"select * from questions limit 5\")\n\n  questionid        creationdate score viewcount answercount\n1   65534165 2021-01-01 22:15:54     0       112           2\n2   65535296 2021-01-02 01:33:13     2      1109           0\n3   65535910 2021-01-02 04:01:34    -1       110           1\n4   65535916 2021-01-02 04:03:20     1        35           1\n5   65536749 2021-01-02 07:03:04     0       108           1\n  commentcount favoritecount                               title\n1            0            NA     Can't update a value in sqlite3\n2            0            NA Install and run ROS on Google Colab\n3            8             0       Operators on date/time fields\n4            0            NA          Plotting values normalised\n5            5            NA     Export C# to word with template\n   ownerid\n1 13189393\n2 14924336\n3   651174\n4 14695007\n5 14899717\n\n\nNow let’s see some more interesting usage of other SQL syntax.\nFirst we get the questions that are viewed the most by filtering to the rows for which the ‘viewcount’ is greater than 100000. We’ll limit the results to the first 5 rows so we don’t print too much out.\n\ndbGetQuery(db, \"select * from questions where viewcount &gt; 100000 limit 5\")\n\n  questionid        creationdate score viewcount answercount\n1   65547199 2021-01-03 06:22:52   124    110832           7\n2   65549858 2021-01-03 12:30:19    52    130479          11\n3   65630743 2021-01-08 14:20:57    77    107140          19\n4   65632698 2021-01-08 16:22:59    74    101044           9\n5   65896334 2021-01-26 05:33:33   111    141899          12\n  commentcount favoritecount\n1            2             0\n2            0             0\n3            4             0\n4            1             0\n5            7             0\n                                                                                  title\n1                                                          Using Bootstrap 5 with Vue 3\n2 \"ERESOLVE unable to resolve dependency tree\" when installing npm react-facebook-login\n3                          How to solve flutter web api cors error only with dart code?\n4                                            How to open a link in a new Tab in NextJS?\n5                              Python Pip broken with sys.stderr.write(f\"ERROR: {exc}\")\n   ownerid\n1 11232893\n2 12425004\n3 12373446\n4  9578961\n5   202576\n\n\nNext, let’s find the number of views for the 15 questions viewed the most.\n\ndbGetQuery(db, \"select viewcount from questions \n                order by viewcount desc limit 15\")\n\n   viewcount\n1    1236876\n2     816368\n3     610026\n4     505992\n5     458856\n6     445775\n7     426798\n8     315861\n9     307961\n10    303399\n11    296364\n12    286886\n13    286810\n14    278432\n15    276806\n\n\nLet’s lay out the various verbs in SQL. Here’s the form of a standard query (but note that the sorting done by ORDER BY is computationally expensive and would be used sparingly):\nSELECT &lt;column(s)&gt; FROM &lt;table&gt; WHERE &lt;condition(s) on column(s)&gt; ORDER BY &lt;column(s)&gt;\nSQL keywords are often written in ALL CAPITALS by convention, although I won’t necessarily do that in this tutorial.\nAnd here is a table of some important keywords:\n\n\n\n\n\n\n\nKeyword\nWhat it does\n\n\n\n\nSELECT\nselect columns\n\n\nFROM\nwhich table to operate on\n\n\nWHERE\nfilter (choose) rows satisfying certain conditions\n\n\nLIKE, IN, &lt;, &gt;, =, &lt;=, &gt;=, !=, etc.\nused as part of filtering conditions\n\n\nORDER BY\nsort based on columns\n\n\n\nSome other keywords are: DISTINCT, ON, JOIN, GROUP BY, AS, USING, UNION, INTERSECT, HAVING, SIMILAR TO (not available in SQLite), SUBSTR in SQLite and SUBSTRING in PostgreSQL.\n\n\n\n\n\n\nChallenge\n\n\n\nReturn a few rows from the users, questions, answers, and tags tables so you can get a sense for what the entries in the tables are like.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nFind the users in the database with the most upvotes.\n\n\n\n\n\nA useful SQL keyword is DISTINCT, which allows you to eliminate duplicate rows from any table (or remove duplicate values when one only has a single column or set of values).\n\n## Find the unique tags:\ndbGetQuery(db, \"select distinct tag from questions_tags limit 15\")\n\n          tag\n1     sorting\n2  visual-c++\n3         mfc\n4   cgridctrl\n5         css\n6      anchor\n7        divi\n8      python\n9  python-3.x\n10      audio\n11        vlc\n12        ios\n13     arrays\n14  dataframe\n15 javascript\n\n## Count the number of unique tags:\ndbGetQuery(db, \"select count(distinct tag) from questions_tags\")\n\n  count(distinct tag)\n1               42137\n\n\n\n\n\nA common pattern of operation is to stratify the dataset, i.e., collect it into mutually exclusive and exhaustive subsets. One would then generally do some aggregation operation on each subset. The aggregation is always done within each of the groups. In SQL this is done with the GROUP BY keyword.\nHere’s a basic example where we count the occurrences of different tags.\n\ndbGetQuery(db, \"select tag, count(*) as n from questions_tags\n                group by tag order by n desc limit 100\")\n\nAlso note the use of as to define a name for the new column.\n\n\n\n\n\n\nChallenge\n\n\n\nWhat specifically does that query do? Describe the table that would be returned.\n\n\nIn general GROUP BY statements will involve some aggregation operation on the subsets. Options include: COUNT, MIN, MAX, AVG, SUM.\nThe result of a query that uses group by is a table with as many rows as groups.\n\n\n\n\n\n\nwhere vs. having\n\n\n\nTo filter the result of a grouping operation, we need to use having rather than where. (Note that where would filter before the application of the group by).\n\ndbGetQuery(db, \"select tag, count(*) as n from questions_tags\n               group by tag having n &gt; 100000 limit 10\")\n\n\n\n\n\n\n\n\n\nFields and group by\n\n\n\nDetermining what fields can be selected when using group by can be tricky, because it varies by DBMS. For example, with Postgres, you can only select fields created by aggregation and the fields that group by is applied to, as well as when there is something called a functional dependency. SQLite allows more flexibility. For example the following can be done in SQLite to find user and answer information for the answer to each question from the user with the highest reputation. However Postgres gives the error ‘ERROR: column “u.userid” must appear in the GROUP BY clause or be used in an aggregate function’.\n\ndbGetQuery(db, \"select *, max(reputation) from users U join answers A\n                on A.ownerid = U.userid group by A.questionid limit 5\")\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that will count the number of answers for each question, returning the IDs of the most answered questions. Hint: consider which field in the “answers” table we do the grouping on (and you shouldn’t need to use the “questions” table).\n\n\n\n\n\n\n\n\ncount and NULL values\n\n\n\nWhen applied to a specific field, COUNT will not count elements that are NULL. That can be useful in cases such as determining the number of non-matches in an outer join. In contrast, COUNT(*) will count the number of rows, regardless of the contents.\n\n\n\n\n\n\n\nSuppose in the example of students in classes, we want a result that has the grades of all students in 9th grade. For this we need information from the Student table (to determine grade level) and information from the ClassAssignment table (to determine the class grade for each class a student takes). Getting information from multiple tables, where a row in one table is matched with one or more rows in another table is called a join. In this case the join would look for all rows in the ClassAssignment table that match a given row (i.e., student) in the Student table, using the column in each of the tables containing the student ID to do the matching of rows.\nThe syntax generally looks like this (again the WHERE and ORDER BY are optional):\nSELECT &lt;column(s)&gt; FROM &lt;table1&gt; JOIN &lt;table2&gt; ON &lt;columns to match on&gt;\n   WHERE &lt;condition(s) on column(s)&gt; ORDER BY &lt;column(s)&gt;\nLet’s see an example join on the Stack Overflow database. In particular let’s select only the questions with the tag “python”.\n\nresult1 &lt;- dbGetQuery(db, \"select * from questions join questions_tags \n                           on questions.questionid = questions_tags.questionid \n                           where tag = 'python'\")\nhead(result1)           \n\n  questionid        creationdate score viewcount answercount\n1   65526804 2021-01-01 01:54:10     0      2087           3\n2   65527402 2021-01-01 05:14:22     1        56           1\n3   65529525 2021-01-01 12:06:43     1       175           1\n4   65529971 2021-01-01 13:14:40     1        39           0\n5   65532644 2021-01-01 18:46:52    -2        49           1\n6   65534179 2021-01-01 22:17:15     1       476           0\n  commentcount favoritecount\n1            3            NA\n2            0            NA\n3            0            NA\n4            1            NA\n5            1            NA\n6            4            NA\n                                                             title\n1            How to play an audio file starting at a specific time\n2                                 Join dataframe columns in python\n3                              Issues with pygame.time.get_ticks()\n4 How to check if Windows prompts a notification box using python?\n5                      How I divide this text file in a Dataframe?\n6                         Suppress OpenCV Output Message in Python\n   ownerid questionid    tag\n1 14718094   65526804 python\n2  1492229   65527402 python\n3 13720770   65529525 python\n4 13845215   65529971 python\n5 14122166   65532644 python\n6 10355409   65534179 python\n\n\nIt’s also possible to get the same exact result without using the JOIN keyword, but you’ll need the WHERE keyword to ensure that the rows get matched correctly.\n\nresult2 &lt;- dbGetQuery(db, \"select * from questions, questions_tags\n                           where questions.questionid = questions_tags.questionid \n                           and tag = 'python'\")\n\n\nidentical(result1, result2)\n\n[1] TRUE\n\n\nWe’ll explain what is going on in the next section.\nHere’s a three-way join (both with and without the JOIN keyword) with some additional use of aliases to abbreviate table names. What does this query ask for?\n\nresult1 &lt;- dbGetQuery(db, \"select * from questions Q\n                           join questions_tags T on Q.questionid = T.questionid\n                           join users U on Q.ownerid = U.userid\n                           where tag = 'python' and upvotes &gt; 100\")\n\nOnce again, we could do that without JOIN and using WHERE to match the rows appropriately.\n\nresult2 &lt;- dbGetQuery(db, \"select * from questions Q, questions_tags T, users U\n                           where Q.questionid = T.questionid \n                           and Q.ownerid = U.userid\n                           and tag = 'python' and upvotes &gt; 100\")\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that would return all the answers to questions with the Python tag.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that will count the number of answers for each question, returning the most answered questions and their information. Note that this extends the question in the previous section.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that would return the users who have answered a question with the Python tag.\n\n\n\n\n\nWe’ve seen a bunch of joins but haven’t discussed the full taxonomy of types of joins. There are various possibilities for how to do a join depending on whether there are rows in one table that do not match any rows in another table.\nInner joins: In database terminology an inner join is when the result has a row for each match of a row in one table with the rows in the second table, where the matching is done on the columns you indicate. If a row in one table corresponds to more than one row in another table, you get all of the matching rows in the second table, with the information from the first table duplicated for each of the resulting rows. For example in the Stack Overflow data, an inner join of questions and answers would pair each question with each of the answers to that question. However, questions without any answers or (if this were possible) answers without a corresponding question would not be part of the result.\nOuter joins: Outer joins add additional rows from one table that do not match any rows from the other table as follows. A left outer join gives all the rows from the first table but only those from the second table that match a row in the first table. A right outer join is the converse, while a full outer join includes at least one copy of all rows from both tables. So a left outer join of the Stack Overflow questions and answers tables would, in addition to the matched questions and their answers, include a row for each question without any answers, as would a full outer join. In this case there should be no answers that do not correspond to question, so a right outer join should be the same as an inner join. Note that one cannot do a right outer join (or a full outer join) in SQLit; you’ll need to switch the order of the tables and do a left outer join.\nCross joins: A cross join gives the Cartesian product of the two tables, namely the pairwise combination of every row from each table, analogous to expand.grid in R. I.e., take a row from the first table and pair it with each row from the second table, then repeat that for all rows from the first table. Since cross joins pair each row in one table with all the rows in another table, the resulting table can be quite large (the product of the number of rows in the two tables). In the Stack Overflow database, a cross join would pair each question with every answer in the database, regardless of whether the answer is an answer to that question.\nHere’s a table of the different kinds of joins:\n\n\n\n\n\n\n\n\nType of join\nRows from first table\nRows from second table\n\n\n\n\ninner (default)\nall that match on specified condition\nall that match on specified condition\n\n\nleft outer\nall\nall that match first table\n\n\nright outer\nall that match second table\nall\n\n\nfull outer\nall\nall\n\n\ncross\nall combined pairwise with second table\nall combined pairwise with first table\n\n\n\nA ‘natural’ join is an inner join that doesn’t require you to specify the common columns between tables on which to enforce equality, but it’s often good practice to not use a natural join and to explicitly indicate which columns are being matched on.\nSimply listing two or more tables separated by commas as we saw earlier is the same as a cross join. Alternatively, listing two or more tables separated by commas, followed by conditions that equate rows in one table to rows in another is the same as an inner join.\nIn general, inner joins can be seen as a form of cross join followed by a condition that enforces matching between the rows of the table. More broadly, here are five equivalent joins that all perform the equivalent of an inner join:\nselect * from table1 join table2 on table1.id = table2.id ## explicit inner join\nselect * from table1, table2 where table1.id = table2.id  ## without explicit JOIN\nselect * from table1 cross join table2 where table1.id = table2.id \nselect * from table1 join table2 using(id)\nselect * from table1 natural join table2\nNote that in the last query the join would be based on all common columns, which could be a bit dangerous if you don’t look carefully at the schema of both tables. Assuming id is the common column, then the last of these queries is the same as the others.\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a view with one row for every question-tag pair, including questions without any tags.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that would return the displaynames of all of the users who have never posted a question. The NULL keyword will come in handy – it’s like NA in R. Hint: NULLs should be produced if you do an outer join.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nHow many questions tagged with ‘random-forest’ were unanswered? (You should need two different kinds of joins to answer this.)\n\n\n\n\n\nWe’ve seen that one can user outer joins as a way to find rows in one table that do not appear in another table. If you want to be able to count the number of entries by group and have a count of 0 when a group doesn’t appear in a table, you can do that by making sure to apply the count only to a field produced by an outer join that contains NULL values and not to all fields.\nFor example if we wanted to count the number of answers provided by each user and make sure to include people who have answered no questions, assigning them a value of 0, we could do it like this by counting the answerid field, which will have NULL for users with no answers and will be counted as a 0:\n\ndbGetQuery(db, \"select userid, count(answerid) as n_answers from users left outer join answers on userid=ownerid group by userid order by userid limit 10\")\n\nIf we had done count(*), then each row for a user with no answers would be incorrectly assigned a 1 (since they have a row associated with them) instead of a 0 (note user #56 below).\n\ndbGetQuery(db, \"select userid, count(*) as n_answers from users left outer join answers on userid=ownerid group by userid order by userid limit 10\")\n\n\n\n\nSometimes we want to query information across rows of the same table. For example supposed we want to analyze the time lags between when the same person posts a question. Do people tend to post in bursts or do they tend to post uniformly over the year? To do this we need contrasts between the times of the different posts. (One can also address this using window functions, discussed later.)\nSo we need to join two copies of the same table, which means dealing with resolving the multiple copies of each column.\nThis would look like this:\n\ndbGetQuery(db, \"select * from questions Q1 join questions Q2\n                on Q1.ownerid = Q2.ownerid\")\n\nThat should create a new table with all pairs of questions asked by a single person.\nActually, there’s a problem here.\n\n\n\n\n\n\nChallenge\n\n\n\nWhat kinds of rows will we get that we don’t want?\n\n\nA solution to that problem of having the same question paired with itself is:\n\ndbGetQuery(db, \"create view question_contrasts as\n                select * from questions Q1 join questions Q2\n                on Q1.ownerid = Q2.ownerid\n                where Q1.creationdate != Q2.creationdate\")\n\n\n\n\n\n\n\nChallenge\n\n\n\nThere’s actually a further similar problem. What is the problem and how can we fix it by changing two characters in the query above? Hint, even as character strings, the creationdate column has an ordering.\n\n\n\n\n\n\nYou can think of a view as a temporary table that is the result of a query and can be used in subsequent queries. In any given query you can use both views and tables. The advantage is that they provide modularity in our querying. For example, if a given operation (portion of a query) is needed repeatedly, one could abstract that as a view and then make use of that view.\nSuppose we always want the upvotes and displayname of question owners available. Once we have the view we can query it like a regular table.\n\n## note there is a creationdate in users too, hence disambiguation\ndbExecute(db, \"create view questions_plus as\n               select questionid, questions.creationdate, score, viewcount, \n                      title, ownerid, upvotes, displayname\n               from questions join users on questions.ownerid = users.userid\")\n\n## don't be confused by the \"0\" response --\n## it just means that nothing is returned to R; the view _has_ been created\n               \ndbGetQuery(db, \"select * from questions_plus where upvotes &gt; 100 limit 5\")\n\nOne use of a view would be to create a mega table that stores all the information from multiple tables in the (unnormalized) form you might have if you simply had one data frame in R or Python.\n\ndbExecute(db, \"drop view questions_plus\") # drop the view if we no longer need it\n\nIf you want to create a temporary table just for a single query, you can use a subquery or a WITH clause, as dicussed in Section 3.2.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "sql.html#introduction-to-sql",
    "href": "sql.html#introduction-to-sql",
    "title": "SQL",
    "section": "",
    "text": "Here is a simple query that selects the first five rows (and all columns, based on the * wildcard) from the questions table.\nselect * from questions limit 5\nTo run this from R we provide the SQL syntax as a string as the second argument to dbGetQuery.\n\nlibrary(RSQLite)\ndrv &lt;- dbDriver(\"SQLite\")\ndir &lt;- 'data' # relative or absolute path to where the .db file is\ndbFilename &lt;- 'stackoverflow-2021.db'\ndb &lt;- dbConnect(drv, dbname = file.path(dir, dbFilename))\ndbGetQuery(db, \"select * from questions limit 5\")\n\n  questionid        creationdate score viewcount answercount\n1   65534165 2021-01-01 22:15:54     0       112           2\n2   65535296 2021-01-02 01:33:13     2      1109           0\n3   65535910 2021-01-02 04:01:34    -1       110           1\n4   65535916 2021-01-02 04:03:20     1        35           1\n5   65536749 2021-01-02 07:03:04     0       108           1\n  commentcount favoritecount                               title\n1            0            NA     Can't update a value in sqlite3\n2            0            NA Install and run ROS on Google Colab\n3            8             0       Operators on date/time fields\n4            0            NA          Plotting values normalised\n5            5            NA     Export C# to word with template\n   ownerid\n1 13189393\n2 14924336\n3   651174\n4 14695007\n5 14899717\n\n\nNow let’s see some more interesting usage of other SQL syntax.\nFirst we get the questions that are viewed the most by filtering to the rows for which the ‘viewcount’ is greater than 100000. We’ll limit the results to the first 5 rows so we don’t print too much out.\n\ndbGetQuery(db, \"select * from questions where viewcount &gt; 100000 limit 5\")\n\n  questionid        creationdate score viewcount answercount\n1   65547199 2021-01-03 06:22:52   124    110832           7\n2   65549858 2021-01-03 12:30:19    52    130479          11\n3   65630743 2021-01-08 14:20:57    77    107140          19\n4   65632698 2021-01-08 16:22:59    74    101044           9\n5   65896334 2021-01-26 05:33:33   111    141899          12\n  commentcount favoritecount\n1            2             0\n2            0             0\n3            4             0\n4            1             0\n5            7             0\n                                                                                  title\n1                                                          Using Bootstrap 5 with Vue 3\n2 \"ERESOLVE unable to resolve dependency tree\" when installing npm react-facebook-login\n3                          How to solve flutter web api cors error only with dart code?\n4                                            How to open a link in a new Tab in NextJS?\n5                              Python Pip broken with sys.stderr.write(f\"ERROR: {exc}\")\n   ownerid\n1 11232893\n2 12425004\n3 12373446\n4  9578961\n5   202576\n\n\nNext, let’s find the number of views for the 15 questions viewed the most.\n\ndbGetQuery(db, \"select viewcount from questions \n                order by viewcount desc limit 15\")\n\n   viewcount\n1    1236876\n2     816368\n3     610026\n4     505992\n5     458856\n6     445775\n7     426798\n8     315861\n9     307961\n10    303399\n11    296364\n12    286886\n13    286810\n14    278432\n15    276806\n\n\nLet’s lay out the various verbs in SQL. Here’s the form of a standard query (but note that the sorting done by ORDER BY is computationally expensive and would be used sparingly):\nSELECT &lt;column(s)&gt; FROM &lt;table&gt; WHERE &lt;condition(s) on column(s)&gt; ORDER BY &lt;column(s)&gt;\nSQL keywords are often written in ALL CAPITALS by convention, although I won’t necessarily do that in this tutorial.\nAnd here is a table of some important keywords:\n\n\n\n\n\n\n\nKeyword\nWhat it does\n\n\n\n\nSELECT\nselect columns\n\n\nFROM\nwhich table to operate on\n\n\nWHERE\nfilter (choose) rows satisfying certain conditions\n\n\nLIKE, IN, &lt;, &gt;, =, &lt;=, &gt;=, !=, etc.\nused as part of filtering conditions\n\n\nORDER BY\nsort based on columns\n\n\n\nSome other keywords are: DISTINCT, ON, JOIN, GROUP BY, AS, USING, UNION, INTERSECT, HAVING, SIMILAR TO (not available in SQLite), SUBSTR in SQLite and SUBSTRING in PostgreSQL.\n\n\n\n\n\n\nChallenge\n\n\n\nReturn a few rows from the users, questions, answers, and tags tables so you can get a sense for what the entries in the tables are like.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nFind the users in the database with the most upvotes.\n\n\n\n\n\nA useful SQL keyword is DISTINCT, which allows you to eliminate duplicate rows from any table (or remove duplicate values when one only has a single column or set of values).\n\n## Find the unique tags:\ndbGetQuery(db, \"select distinct tag from questions_tags limit 15\")\n\n          tag\n1     sorting\n2  visual-c++\n3         mfc\n4   cgridctrl\n5         css\n6      anchor\n7        divi\n8      python\n9  python-3.x\n10      audio\n11        vlc\n12        ios\n13     arrays\n14  dataframe\n15 javascript\n\n## Count the number of unique tags:\ndbGetQuery(db, \"select count(distinct tag) from questions_tags\")\n\n  count(distinct tag)\n1               42137\n\n\n\n\n\nA common pattern of operation is to stratify the dataset, i.e., collect it into mutually exclusive and exhaustive subsets. One would then generally do some aggregation operation on each subset. The aggregation is always done within each of the groups. In SQL this is done with the GROUP BY keyword.\nHere’s a basic example where we count the occurrences of different tags.\n\ndbGetQuery(db, \"select tag, count(*) as n from questions_tags\n                group by tag order by n desc limit 100\")\n\nAlso note the use of as to define a name for the new column.\n\n\n\n\n\n\nChallenge\n\n\n\nWhat specifically does that query do? Describe the table that would be returned.\n\n\nIn general GROUP BY statements will involve some aggregation operation on the subsets. Options include: COUNT, MIN, MAX, AVG, SUM.\nThe result of a query that uses group by is a table with as many rows as groups.\n\n\n\n\n\n\nwhere vs. having\n\n\n\nTo filter the result of a grouping operation, we need to use having rather than where. (Note that where would filter before the application of the group by).\n\ndbGetQuery(db, \"select tag, count(*) as n from questions_tags\n               group by tag having n &gt; 100000 limit 10\")\n\n\n\n\n\n\n\n\n\nFields and group by\n\n\n\nDetermining what fields can be selected when using group by can be tricky, because it varies by DBMS. For example, with Postgres, you can only select fields created by aggregation and the fields that group by is applied to, as well as when there is something called a functional dependency. SQLite allows more flexibility. For example the following can be done in SQLite to find user and answer information for the answer to each question from the user with the highest reputation. However Postgres gives the error ‘ERROR: column “u.userid” must appear in the GROUP BY clause or be used in an aggregate function’.\n\ndbGetQuery(db, \"select *, max(reputation) from users U join answers A\n                on A.ownerid = U.userid group by A.questionid limit 5\")\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that will count the number of answers for each question, returning the IDs of the most answered questions. Hint: consider which field in the “answers” table we do the grouping on (and you shouldn’t need to use the “questions” table).\n\n\n\n\n\n\n\n\ncount and NULL values\n\n\n\nWhen applied to a specific field, COUNT will not count elements that are NULL. That can be useful in cases such as determining the number of non-matches in an outer join. In contrast, COUNT(*) will count the number of rows, regardless of the contents.\n\n\n\n\n\n\n\nSuppose in the example of students in classes, we want a result that has the grades of all students in 9th grade. For this we need information from the Student table (to determine grade level) and information from the ClassAssignment table (to determine the class grade for each class a student takes). Getting information from multiple tables, where a row in one table is matched with one or more rows in another table is called a join. In this case the join would look for all rows in the ClassAssignment table that match a given row (i.e., student) in the Student table, using the column in each of the tables containing the student ID to do the matching of rows.\nThe syntax generally looks like this (again the WHERE and ORDER BY are optional):\nSELECT &lt;column(s)&gt; FROM &lt;table1&gt; JOIN &lt;table2&gt; ON &lt;columns to match on&gt;\n   WHERE &lt;condition(s) on column(s)&gt; ORDER BY &lt;column(s)&gt;\nLet’s see an example join on the Stack Overflow database. In particular let’s select only the questions with the tag “python”.\n\nresult1 &lt;- dbGetQuery(db, \"select * from questions join questions_tags \n                           on questions.questionid = questions_tags.questionid \n                           where tag = 'python'\")\nhead(result1)           \n\n  questionid        creationdate score viewcount answercount\n1   65526804 2021-01-01 01:54:10     0      2087           3\n2   65527402 2021-01-01 05:14:22     1        56           1\n3   65529525 2021-01-01 12:06:43     1       175           1\n4   65529971 2021-01-01 13:14:40     1        39           0\n5   65532644 2021-01-01 18:46:52    -2        49           1\n6   65534179 2021-01-01 22:17:15     1       476           0\n  commentcount favoritecount\n1            3            NA\n2            0            NA\n3            0            NA\n4            1            NA\n5            1            NA\n6            4            NA\n                                                             title\n1            How to play an audio file starting at a specific time\n2                                 Join dataframe columns in python\n3                              Issues with pygame.time.get_ticks()\n4 How to check if Windows prompts a notification box using python?\n5                      How I divide this text file in a Dataframe?\n6                         Suppress OpenCV Output Message in Python\n   ownerid questionid    tag\n1 14718094   65526804 python\n2  1492229   65527402 python\n3 13720770   65529525 python\n4 13845215   65529971 python\n5 14122166   65532644 python\n6 10355409   65534179 python\n\n\nIt’s also possible to get the same exact result without using the JOIN keyword, but you’ll need the WHERE keyword to ensure that the rows get matched correctly.\n\nresult2 &lt;- dbGetQuery(db, \"select * from questions, questions_tags\n                           where questions.questionid = questions_tags.questionid \n                           and tag = 'python'\")\n\n\nidentical(result1, result2)\n\n[1] TRUE\n\n\nWe’ll explain what is going on in the next section.\nHere’s a three-way join (both with and without the JOIN keyword) with some additional use of aliases to abbreviate table names. What does this query ask for?\n\nresult1 &lt;- dbGetQuery(db, \"select * from questions Q\n                           join questions_tags T on Q.questionid = T.questionid\n                           join users U on Q.ownerid = U.userid\n                           where tag = 'python' and upvotes &gt; 100\")\n\nOnce again, we could do that without JOIN and using WHERE to match the rows appropriately.\n\nresult2 &lt;- dbGetQuery(db, \"select * from questions Q, questions_tags T, users U\n                           where Q.questionid = T.questionid \n                           and Q.ownerid = U.userid\n                           and tag = 'python' and upvotes &gt; 100\")\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that would return all the answers to questions with the Python tag.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that will count the number of answers for each question, returning the most answered questions and their information. Note that this extends the question in the previous section.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that would return the users who have answered a question with the Python tag.\n\n\n\n\n\nWe’ve seen a bunch of joins but haven’t discussed the full taxonomy of types of joins. There are various possibilities for how to do a join depending on whether there are rows in one table that do not match any rows in another table.\nInner joins: In database terminology an inner join is when the result has a row for each match of a row in one table with the rows in the second table, where the matching is done on the columns you indicate. If a row in one table corresponds to more than one row in another table, you get all of the matching rows in the second table, with the information from the first table duplicated for each of the resulting rows. For example in the Stack Overflow data, an inner join of questions and answers would pair each question with each of the answers to that question. However, questions without any answers or (if this were possible) answers without a corresponding question would not be part of the result.\nOuter joins: Outer joins add additional rows from one table that do not match any rows from the other table as follows. A left outer join gives all the rows from the first table but only those from the second table that match a row in the first table. A right outer join is the converse, while a full outer join includes at least one copy of all rows from both tables. So a left outer join of the Stack Overflow questions and answers tables would, in addition to the matched questions and their answers, include a row for each question without any answers, as would a full outer join. In this case there should be no answers that do not correspond to question, so a right outer join should be the same as an inner join. Note that one cannot do a right outer join (or a full outer join) in SQLit; you’ll need to switch the order of the tables and do a left outer join.\nCross joins: A cross join gives the Cartesian product of the two tables, namely the pairwise combination of every row from each table, analogous to expand.grid in R. I.e., take a row from the first table and pair it with each row from the second table, then repeat that for all rows from the first table. Since cross joins pair each row in one table with all the rows in another table, the resulting table can be quite large (the product of the number of rows in the two tables). In the Stack Overflow database, a cross join would pair each question with every answer in the database, regardless of whether the answer is an answer to that question.\nHere’s a table of the different kinds of joins:\n\n\n\n\n\n\n\n\nType of join\nRows from first table\nRows from second table\n\n\n\n\ninner (default)\nall that match on specified condition\nall that match on specified condition\n\n\nleft outer\nall\nall that match first table\n\n\nright outer\nall that match second table\nall\n\n\nfull outer\nall\nall\n\n\ncross\nall combined pairwise with second table\nall combined pairwise with first table\n\n\n\nA ‘natural’ join is an inner join that doesn’t require you to specify the common columns between tables on which to enforce equality, but it’s often good practice to not use a natural join and to explicitly indicate which columns are being matched on.\nSimply listing two or more tables separated by commas as we saw earlier is the same as a cross join. Alternatively, listing two or more tables separated by commas, followed by conditions that equate rows in one table to rows in another is the same as an inner join.\nIn general, inner joins can be seen as a form of cross join followed by a condition that enforces matching between the rows of the table. More broadly, here are five equivalent joins that all perform the equivalent of an inner join:\nselect * from table1 join table2 on table1.id = table2.id ## explicit inner join\nselect * from table1, table2 where table1.id = table2.id  ## without explicit JOIN\nselect * from table1 cross join table2 where table1.id = table2.id \nselect * from table1 join table2 using(id)\nselect * from table1 natural join table2\nNote that in the last query the join would be based on all common columns, which could be a bit dangerous if you don’t look carefully at the schema of both tables. Assuming id is the common column, then the last of these queries is the same as the others.\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a view with one row for every question-tag pair, including questions without any tags.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that would return the displaynames of all of the users who have never posted a question. The NULL keyword will come in handy – it’s like NA in R. Hint: NULLs should be produced if you do an outer join.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nHow many questions tagged with ‘random-forest’ were unanswered? (You should need two different kinds of joins to answer this.)\n\n\n\n\n\nWe’ve seen that one can user outer joins as a way to find rows in one table that do not appear in another table. If you want to be able to count the number of entries by group and have a count of 0 when a group doesn’t appear in a table, you can do that by making sure to apply the count only to a field produced by an outer join that contains NULL values and not to all fields.\nFor example if we wanted to count the number of answers provided by each user and make sure to include people who have answered no questions, assigning them a value of 0, we could do it like this by counting the answerid field, which will have NULL for users with no answers and will be counted as a 0:\n\ndbGetQuery(db, \"select userid, count(answerid) as n_answers from users left outer join answers on userid=ownerid group by userid order by userid limit 10\")\n\nIf we had done count(*), then each row for a user with no answers would be incorrectly assigned a 1 (since they have a row associated with them) instead of a 0 (note user #56 below).\n\ndbGetQuery(db, \"select userid, count(*) as n_answers from users left outer join answers on userid=ownerid group by userid order by userid limit 10\")\n\n\n\n\nSometimes we want to query information across rows of the same table. For example supposed we want to analyze the time lags between when the same person posts a question. Do people tend to post in bursts or do they tend to post uniformly over the year? To do this we need contrasts between the times of the different posts. (One can also address this using window functions, discussed later.)\nSo we need to join two copies of the same table, which means dealing with resolving the multiple copies of each column.\nThis would look like this:\n\ndbGetQuery(db, \"select * from questions Q1 join questions Q2\n                on Q1.ownerid = Q2.ownerid\")\n\nThat should create a new table with all pairs of questions asked by a single person.\nActually, there’s a problem here.\n\n\n\n\n\n\nChallenge\n\n\n\nWhat kinds of rows will we get that we don’t want?\n\n\nA solution to that problem of having the same question paired with itself is:\n\ndbGetQuery(db, \"create view question_contrasts as\n                select * from questions Q1 join questions Q2\n                on Q1.ownerid = Q2.ownerid\n                where Q1.creationdate != Q2.creationdate\")\n\n\n\n\n\n\n\nChallenge\n\n\n\nThere’s actually a further similar problem. What is the problem and how can we fix it by changing two characters in the query above? Hint, even as character strings, the creationdate column has an ordering.\n\n\n\n\n\n\nYou can think of a view as a temporary table that is the result of a query and can be used in subsequent queries. In any given query you can use both views and tables. The advantage is that they provide modularity in our querying. For example, if a given operation (portion of a query) is needed repeatedly, one could abstract that as a view and then make use of that view.\nSuppose we always want the upvotes and displayname of question owners available. Once we have the view we can query it like a regular table.\n\n## note there is a creationdate in users too, hence disambiguation\ndbExecute(db, \"create view questions_plus as\n               select questionid, questions.creationdate, score, viewcount, \n                      title, ownerid, upvotes, displayname\n               from questions join users on questions.ownerid = users.userid\")\n\n## don't be confused by the \"0\" response --\n## it just means that nothing is returned to R; the view _has_ been created\n               \ndbGetQuery(db, \"select * from questions_plus where upvotes &gt; 100 limit 5\")\n\nOne use of a view would be to create a mega table that stores all the information from multiple tables in the (unnormalized) form you might have if you simply had one data frame in R or Python.\n\ndbExecute(db, \"drop view questions_plus\") # drop the view if we no longer need it\n\nIf you want to create a temporary table just for a single query, you can use a subquery or a WITH clause, as dicussed in Section 3.2.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "sql.html#additional-sql-topics",
    "href": "sql.html#additional-sql-topics",
    "title": "SQL",
    "section": "2 Additional SQL topics",
    "text": "2 Additional SQL topics\n\n2.1 Creating database tables\nOften one would create tables from within R or Python (though one can create tables from within the sqlite and psql command line interfaces as well). Here’s the syntax from R.\n\n## Option 1: pass directly from CSV to database\ndbWriteTable(conn = db, name = \"student\", value = \"student.csv\", row.names = FALSE,\n             header = TRUE)\n\n## Option 2: pass from data in an R data frame\n## First create your data frame:\n# student_df &lt;- data.frame(...)\n## or\n# student_df &lt;- read.csv(...)\ndbWriteTable(conn = db, name = \"student\", value = student_df, row.names = FALSE,\n             append = FALSE)\n\n\n\n2.2 String processing and creating new fields\nWe can do some basic matching with LIKE, using % as a wildcard and _ to stand in for any single character:\n\ndbGetQuery(db, \"select * from questions_tags where tag like 'r-%' limit 10\")\n\n   questionid        tag\n1    65598394   r-factor\n2    65999758 r-markdown\n3    66007924    r-exams\n4    66036936 r-markdown\n5    65985449    r-caret\n6    66035257 r-markdown\n7    66135867 r-markdown\n8    65878099 r-markdown\n9    65973815 r-markdown\n10   66102594   r-lavaan\n\n\nIn Postgres, in addition to the basic use of LIKE to match character strings, one can use regular expression syntax with SIMILAR TO.\nSIMILAR TO is not available in SQLite so the following can only be done in the Postgres instance of our example database. Here we’ll look for all tags that are of the form “r-”, “-r”, “r” or “-r-”. SQL uses % as a wildcard (this is not standard regular expression syntax).\n\n## Try in postgreSQL, not SQLite\nresult &lt;- dbGetQuery(db, \"select * from questions_tags \n                          where tag similar to 'r-%|%-r|r|%-r-%' limit 10\")\n## Standard regex for 'any character' doesn't seem to work:\n## result &lt;- dbGetQuery(db, \"select * from questions_tags \n                             where tag SIMILAR TO 'r-.*|.*-r|r|.*-r-.*' limit 10\")\n\n\n\n\n\n\n\nWarning\n\n\n\nThe matching does not match on substrings, unless one uses wildcards at beginning and end of the pattern, so “r” will only find “r” and not, for example, “dplyr”.\n\n\nTo extract substrings we can SUBSTRING in Postgres. Here’s a basic example:\n\ndbGetQuery(db, \"select substring(creationdate from '^[[:digit:]]{4}') as year\n                from questions limit 3\")\n\nIf you need to specify the pattern to be extracted relative to the surrounding characters, then Postgres requires that the pattern to be extracted be surrounded by #\" (one could use another character in place of #), but for use from R we need to escape the double-quote with a backslash so it is treated as a part of the string passed to Postgres and not treated by R as indicating where the character string stops/starts. We also need the % wildcard character when extracting in this way.\n\ndbGetQuery(db, \"select substring(creationdate from\n                '%-#\\\"[[:digit:]]{2}#\\\"-%' for '#') as month\n                from questions limit 3\")\n\n\n\n\n\n\n\nSubstrings\n\n\n\nSQLite provides SUBSTR for substrings, but the flexibility of SUBSTR seems to be much less than use of SUBSTRING in PostgreSQL.\n\n\nHere is some documentation on string functions in PostgreSQL.\n\n\n\n\n\n\nChallenge\n\n\n\nSelect the questions that have “java” but not “javascript” in their titles using regular expression syntax.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nFigure out how to calculate the length (in characters) of the title of each question.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nProcess the creationdate field to create year, day, and month fields in a new view. Note that this would be good practice for string manipulation but you would want to handle dates and times using the material in the next section and not use string processing.\n\n\n\n\n2.3 Dates and times\nHere we’ll see how you can work with dates and times in SQLite, but the functionality should be similar in other DBMS.\nSQLite doesn’t have specific date-time types, but it’s standard to store date-times as strings in the text field in the ISO-8601 format: YYYY-MM-DD HH:MM:SS.SSS. That’s the format of the dates in the StackOverflow database:\n\ndbGetQuery(db, \"select distinct creationdate from questions limit 5\")\n\n         creationdate\n1 2021-01-01 22:15:54\n2 2021-01-02 01:33:13\n3 2021-01-02 04:01:34\n4 2021-01-02 04:03:20\n5 2021-01-02 07:03:04\n\n\nThen SQLite provides some powerful functions for manipulating and extracting information in such fields. Here are just a few examples, noting that strftime is particularly powerful. Other DBMS should have similar functionality, but I haven’t investigated further.\n\n## Julian days (decimal days since noon UTC/Greenwich time November 24, 4714 BC (Yikes!)). \noutput &lt;- dbGetQuery(db, \"select creationdate, julianday(creationdate)\n                from questions limit 5\")\noutput\n\n         creationdate julianday(creationdate)\n1 2021-01-01 22:15:54                 2459216\n2 2021-01-02 01:33:13                 2459217\n3 2021-01-02 04:01:34                 2459217\n4 2021-01-02 04:03:20                 2459217\n5 2021-01-02 07:03:04                 2459217\n\n## Julian day is decimal-valued:\nformatC(output[ , 2], 6, format = 'f')\n\n[1] \"2459216.427708\" \"2459216.564734\" \"2459216.667755\"\n[4] \"2459216.668981\" \"2459216.793796\"\n\n## Convert to local time\ndbGetQuery(db, \"select distinct creationdate, datetime(creationdate, 'localtime')\n                from questions limit 5\")\n\n         creationdate datetime(creationdate, 'localtime')\n1 2021-01-01 22:15:54                 2021-01-01 14:15:54\n2 2021-01-02 01:33:13                 2021-01-01 17:33:13\n3 2021-01-02 04:01:34                 2021-01-01 20:01:34\n4 2021-01-02 04:03:20                 2021-01-01 20:03:20\n5 2021-01-02 07:03:04                 2021-01-01 23:03:04\n\n## Eastern time, manually, ignoring daylight savings\ndbGetQuery(db, \"select distinct creationdate, datetime(creationdate, '-05:00')\n                from questions limit 5\")\n\n         creationdate datetime(creationdate, '-05:00')\n1 2021-01-01 22:15:54              2021-01-01 17:15:54\n2 2021-01-02 01:33:13              2021-01-01 20:33:13\n3 2021-01-02 04:01:34              2021-01-01 23:01:34\n4 2021-01-02 04:03:20              2021-01-01 23:03:20\n5 2021-01-02 07:03:04              2021-01-02 02:03:04\n\n## day of week: Jan 1 2021 was a Friday (0=Sunday, 6=Saturday)\ndbGetQuery(db, \"select creationdate, strftime('%w', creationdate)\n                from questions limit 5\")\n\n         creationdate strftime('%w', creationdate)\n1 2021-01-01 22:15:54                            5\n2 2021-01-02 01:33:13                            6\n3 2021-01-02 04:01:34                            6\n4 2021-01-02 04:03:20                            6\n5 2021-01-02 07:03:04                            6\n\n\nUnfortunately I’m not sure if the actual dates in the database are Greenwich time or some US time zone, but we’ll ignore that complication here.\nLet’s put it all together to do something meaningful.\n\nresult &lt;- dbGetQuery(db, \"select strftime('%H', creationdate) as hour,\n                          count() as n from questions group by hour\")\nhead(result)\n\n  hour     n\n1   00 35444\n2   01 33989\n3   02 35542\n4   03 37745\n5   04 39609\n6   05 45229\n\nplot(as.numeric(result$hour), result$n, xlab = 'hour of day (UTC/Greenwich???)',\n                                        ylab = 'number of questions')\n\n\n\n\n\n\n\n\nHere’s some documentation of the syntax for the functions, including stftime.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "sql.html#more-advanced-sql",
    "href": "sql.html#more-advanced-sql",
    "title": "SQL",
    "section": "3 More advanced SQL",
    "text": "3 More advanced SQL\n\n3.1 Set operations: UNION, INTERSECT, EXCEPT\nYou can do set operations like union, intersection, and set difference using the UNION, INTERSECT, and EXCEPT keywords on tables that have the same schema (same column names and types), though most often these would be used on single columns (i.e., single-column tables).\n\n\n\n\n\n\nNote\n\n\n\nWhile one can often set up an equivalent query without using INTERSECT or UNION, set operations can be very handy.\n\n\nHere’s an example of a query that can be done with or without an intersection. Suppose we want to know the names of all individuals who have asked both an R question and a Python question. We can do this with INTERSECT:\n\nsystem.time(\n   result1 &lt;- dbGetQuery(db, \"select displayname, userid from\n                              questions Q join users U on U.userid = Q.ownerid\n                              join questions_tags T on Q.questionid = T.questionid\n                              where tag = 'r'\n                              intersect\n                              select displayname, userid from\n                              questions Q join users U on U.userid = Q.ownerid\n                              join questions_tags T on Q.questionid = T.questionid\n                              where tag = 'python'\")\n               )\n\n   user  system elapsed \n  4.239   1.434   7.565 \n\n\nAlternatively we can do a self-join. Note that the syntax gets complicated as we are doing multiple joins.\n\nsystem.time(\n   result2 &lt;- dbGetQuery(db, \"select displayname, userid from\n                              (questions Q1 join questions_tags T1\n                              on Q1.questionid = T1.questionid)\n                              join\n                              (questions Q2 join questions_tags T2\n                              on Q2.questionid = T2.questionid)\n                              on Q1.ownerid = Q2.ownerid\n                              join users on Q1.ownerid = users.userid\n                              where T1.tag = 'r' and T2.tag = 'python'\")\n               )\n\n   user  system elapsed \n  5.977   3.565  10.854 \n\nidentical(result1, result2)\n\n[1] FALSE\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nThose two queries return equivalent information, but the results are not exactly the same. What causes the difference? How can we modify the second query to get the exact same results as the first?\n\n\nWhich is faster? The second one looks more involved in terms of the joins, so the timing results seen above make sense.\nWe could use UNION or EXCEPT to find people who have asked either or only one type of question, respectively.\n\n\n\n\n\n\nChallenge\n\n\n\nFind the users who have asked either an R question or a Python question.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nFind the users who have asked but not answered a question.\n\n\n\n\n3.2 Subqueries\nA subquery is a full query that is embedded in a larger query.\n\n3.2.1 Subqueries in the FROM statement\nWe can use subqueries in the FROM statement to create a temporary table to use in a query. Here we’ll do it in the context of a join.\nTry to figure out what the following query does.\n\ndbGetQuery(db, \"select * from questions join answers A\n                on questions.questionid = A.questionid\n                join\n                (select ownerid, count(*) as n_answered from answers\n                group by ownerid order by n_answered desc limit 1000) most_responsive\n                on A.ownerid = most_responsive.ownerid\")\n\nHere the subquery is select ownerid, count(*) as n_answered from answers group by ownerid order by n_answered desc limit 1000, which creates a temporary table, named most_responsive, having the ownerid (and the number of their answers) for the 1000 users who have answered the most questions.\nIn the ‘outer’ main query, that temporary table is joined to the questions and answers tables.\nIt might be hard to just come up with that full query all at once. A good strategy is probably to think about creating a view that is the result of the inner query and then have the outer query use that. You can then piece together the complicated query in a modular way. For big databases, you are likely to want to submit this as a single query and not two queries so that the SQL optimizer can determine the best way to do the operations. But you want to start with code that you’re confident will give you the right answer!\nNote we could also have done that query using a subquery in the WHERE statement, as discussed in the next section.\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that finds the number of answers per question, but only answers from users with at least 100 upvotes.\n\n\nFinally one can use subqueries in the SELECT clause to create new variables, but we won’t go into that here.\n\n\n3.2.2 Subqueries in the WHERE statement\nInstead of a join, we can use subqueries as a way to combine information across tables, with the subquery involved in a WHERE statement. The subquery creates a set and we then can check for inclusion in (or exclusion from with not in) that set.\nFor example, suppose we want to know the average number of UpVotes for users who have posted a question with the tag “python”.\n\ndbGetQuery(db, \"select avg(upvotes) from users where userid in\n                (select distinct ownerid from\n                questions join questions_tags\n                on questions.questionid = questions_tags.questionid\n                where tag = 'python')\")       \n\n  avg(upvotes)\n1     62.72529\n\n\nIn some cases one can do a join rather than using a subquery, but in the following example, it fails.\n\n\n\n\n\n\nChallenge\n\n\n\nWhat’s wrong with the following query as an attempt to answer the question above? (See if you can figure it out before looking at the answer below.)\n\ndbGetQuery(db, \"select avg(upvotes) from questions, questions_tags, users\n                where questions.questionid = questions_tags.questionid and\n                questions.ownerid = users.userid and\n                tag = 'python'\")\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the subquery, we find the IDs of the users we are looking for and then average over the UpVotes of those individuals. In the join version we found all the questions that had a Python tag and averaged over the UpVotes of the individuals associated with those questions. So the latter includes multiple UpVotes values from individuals who have posted multiple Python questions.\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWrite a query that would return the user information for users who have answered a question with the Python tag. We’ve seen this challenge before, but do it now based on a subquery.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nFind the users who have asked but not answered a question. We’ve seen this before, but now make use of subqueries instead of a join.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nHow would you find all the answers associated with the user with the most upvotes?\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a frequency list of the tags used in the top 100 most answered questions. Note there is a way to do this with a JOIN and a way without a JOIN.\n\n\n\n\n3.2.3 Using WITH\nThe WITH clause allows you to create a temporary table to then use in an associated SELECT statement. So it provides similar functionality to using a view but without it being a persistent part of the database. The temporary table is only available within the associated SELECT statement. WITH can only occur as part of a query with SELECT.\nLet’s see use of WITH to accomplish what we did with a subquery in the FROM statement above.\n\ndbGetQuery(db, \"with most_responsive as (\n                select ownerid, count(*) as n_answered from answers\n                group by ownerid order by n_answered desc limit 1000\n                )\n                select * from questions join answers A\n                on questions.questionid = A.questionid\n                join most_responsive on A.ownerid = most_responsive.ownerid\")\n\nOne could also replace the subquery in the WHERE statement above using WITH.\n\ndbGetQuery(db, \"with tmp as (select distinct ownerid from\n                questions join questions_tags\n                on questions.questionid = questions_tags.questionid\n                where tag = 'python')\n                select avg(UpVotes) from users where userid in\n                tmp\")       \n\nFinally, you can create multiple temporary tables in the WITH clause, separated by commas. This can help make your query more modular without the complication of creating views that will only be used once.\n\n\n\n3.3 Window functions\nWindow functions provide the ability to perform calculations across sets of rows that are related to the current query row.\nComments:\n\nThe result of applying a window function is the same number of rows as the input, even though the functionality is similar to group by. Hint: think about the result of group by + mutate in dplyr in R.\nOne can apply a window function within groups or across the whole table.\nThe functions one can apply include standard aggregation functions such as avg and count as well as non-standard functions (specific to using window functions) such as rank, row_number, and cume_dist.\nUnless you’re simply grouping into categories, you’ll generally need to order the rows for the window function to make sense.\n\nThe syntax is a bit involved, so let’s see with a range of examples:\n\nAggregate within groups but with one output value per input row\n\n\n## Total number of questions for each owner\ndbGetQuery(db, \"select ownerid,\n                count() over (partition by ownerid) as n\n                from questions where ownerid is not NULL limit 10\")\n\n   ownerid n\n1       33 1\n2       51 1\n3       56 3\n4       56 3\n5       56 3\n6       58 2\n7       58 2\n8       95 3\n9       95 3\n10      95 3\n\n\n\nCompute cumulative calculations; note the need for ORDER BY within the PARTITION clause (the other ORDER BY is just for display purposes here):\n\n\n## Rank (based on ordering by creationdate) of questions by owner\ndbGetQuery(db, \"select *,\n                rank() over (partition by ownerid order by creationdate) as rank\n                from questions order by ownerid desc limit 10\")\n\n   questionid        creationdate score viewcount answercount\n1    70347322 2021-12-14 08:47:41     1       123           0\n2    65620082 2021-01-07 21:19:16     0        44           1\n3    65671366 2021-01-11 17:04:26     0        40           1\n4    66034594 2021-02-03 19:43:00     1       111           2\n5    66237702 2021-02-17 07:49:58     0       174           2\n6    66547208 2021-03-09 12:49:23     2       307           1\n7    66907039 2021-04-01 15:07:28     3      1540           1\n8    67099166 2021-04-14 21:11:18     1       289           1\n9    67355292 2021-05-02 10:28:59    -2      5001           1\n10   67374899 2021-05-03 19:42:10     1        66           0\n   commentcount favoritecount\n1             5            NA\n2             9            NA\n3             0            NA\n4             0            NA\n5             0            NA\n6             0            NA\n7             0            NA\n8             0            NA\n9             5            NA\n10            7            NA\n                                                                                                  title\n1                                            Adjust brightness of Arduino's built-in LED with pyFirmata\n2             How to resolve squiggly lines in HTML files after importing Bootstrap to Angular project?\n3                                               Input form fields are cannot be updated in Angular app?\n4                                                      Select first largest value using TOP 1 in MySQL?\n5                                                                Cron expression in Spring, validation?\n6  Playing with Strategy Design Pattern using lambda expression by following Venkat Subramaniam's book?\n7                                                      GraphQL and Spring Security using @PreAuthorize?\n8                                           Emitting from Flux&lt;String&gt; until one of conditions are met?\n9                                        Cannot open older Angular project with latest Angular version?\n10 How to make vertical scrollbar's height same as window's height and make horizontal scrollbar fixed?\n    ownerid rank\n1  20674445    1\n2  20390023    1\n3  20390023    2\n4  20390023    3\n5  20390023    4\n6  20390023    5\n7  20390023    6\n8  20390023    7\n9  20390023    8\n10 20390023    9\n\n\n(Sidenote: we rely here on the fact that ordering alphabetically by creationdate is equivalent to time ordering.)\n\nDo a lagged analysis\n\n\n## Get previous value (based on ordering by creationdate) by owner\ndbGetQuery(db, \"select ownerid, creationdate,\n                lag(creationdate, 1) over\n                (partition by ownerid order by creationdate)\n                as previous_date\n                from questions order by ownerid desc limit 5\")\n\n   ownerid        creationdate       previous_date\n1 20674445 2021-12-14 08:47:41                &lt;NA&gt;\n2 20390023 2021-01-07 21:19:16                &lt;NA&gt;\n3 20390023 2021-01-11 17:04:26 2021-01-07 21:19:16\n4 20390023 2021-02-03 19:43:00 2021-01-11 17:04:26\n5 20390023 2021-02-17 07:49:58 2021-02-03 19:43:00\n\n\nSo one could now calculate the difference between the previous and current date to analyze the time gaps between users posting questions.\n\nDo an analysis within an arbitrary window of rows based on the values in one of the columns\n\n\n## Summarize questions within 15 days of current question \ndbGetQuery(db, \"select ownerid, creationdate,\n                count() over\n                (partition by ownerid order by julianday(creationdate)\n                range between 15 preceding and 15 following)\n                as n_window\n                from questions where ownerid is not null limit 30\")\n\n   ownerid        creationdate n_window\n1       33 2021-06-10 07:58:44        1\n2       51 2021-06-09 18:07:55        1\n3       56 2021-04-21 10:20:45        1\n4       56 2021-11-23 09:40:20        2\n5       56 2021-12-07 14:19:36        2\n6       58 2021-01-22 19:30:22        1\n7       58 2021-06-09 14:56:50        1\n8       95 2021-02-11 19:52:55        1\n9       95 2021-07-21 13:22:25        1\n10      95 2021-10-15 07:28:39        1\n11     101 2021-03-16 18:53:19        1\n12     114 2021-03-11 16:30:45        1\n13     116 2021-02-15 21:58:48        2\n14     116 2021-02-25 03:07:55        2\n15     116 2021-03-24 16:10:27        1\n16     116 2021-04-08 20:48:23        2\n17     116 2021-04-15 19:03:33        2\n18     116 2021-07-12 00:37:47        1\n19     159 2021-03-19 21:48:17        1\n20     159 2021-05-21 17:29:36        2\n21     159 2021-05-26 21:24:16        2\n22     188 2021-08-04 01:48:51        1\n23     199 2021-01-28 23:04:47        1\n24     199 2021-06-07 08:10:25        1\n25     199 2021-10-05 10:10:37        1\n26     199 2021-12-17 23:29:05        1\n27     214 2021-04-29 14:39:04        1\n28     230 2021-03-26 16:30:09        2\n29     230 2021-04-09 13:14:16        2\n30     234 2021-01-04 01:02:03        1\n\n\nThere the ‘15 preceding’ and ‘15 following’ mean to include all rows within each ownerid that are within 15 Julian days (based on ‘creationdate’) of each row.\nSo one could now analyze bursts of activity.\nOne can also choose a fixed number of rows by replacing ‘range’ with ‘rows’. The ROWS and RANGE syntax allow one to specify the window frame in more flexible ways than simply the categories of a categorical variable.\nRanking becomes more complicated when there are ties. RANK will assign the same value to any ties, and then increment based on the number of ties, e.g., you can get 1, 1, 1, 4, if the three lowest value are tied. DENSE RANK will avoid skipping values, giving 1, 1, 1, 2 in the same situation. ROW_NUMBER just numbers, ignoring ties and resulting in ambiguity when there are ties, giving 1, 2, 3, 4 in the same situation.\nSo the syntax of a window function will generally have these elements:\n\na call to some function that calculates within the window and assigns value(s) to the rows in the window\nOVER\nPARTITION BY (optional)\nORDER BY (optional)\nRANGE or ROW (optional)\nAS (optional)\n\nYou can also name window functions, which comes in handy if you want multiple functions applied to the same window:\n\ndbGetQuery(db, \"select ownerid, creationdate,\n                lag(creationdate, 1) over w as lag1,\n                lag(creationdate, 2) over w as lag2\n                from questions where ownerid is not null\n                window w as (partition by ownerid order by creationdate)\n                order by ownerid limit 5\")\n\n  ownerid        creationdate                lag1\n1      33 2021-06-10 07:58:44                &lt;NA&gt;\n2      51 2021-06-09 18:07:55                &lt;NA&gt;\n3      56 2021-04-21 10:20:45                &lt;NA&gt;\n4      56 2021-11-23 09:40:20 2021-04-21 10:20:45\n5      56 2021-12-07 14:19:36 2021-11-23 09:40:20\n                 lag2\n1                &lt;NA&gt;\n2                &lt;NA&gt;\n3                &lt;NA&gt;\n4                &lt;NA&gt;\n5 2021-04-21 10:20:45\n\n\nWhat does that query do?\nFinally, you can use window functions on the entire table, without partitioning.\n\n## Summarize questions within 15 (decimal) days of current question \ndbGetQuery(db, \"select ownerid, creationdate,\n                count() over\n                (order by julianday(creationdate)\n                range between 15 preceding and 15 following)\n                as n_window\n                from questions where ownerid is not null limit 30\")\n\n    ownerid        creationdate n_window\n1  13708122 2021-01-01 00:00:01    65899\n2  14920712 2021-01-01 00:00:59    65901\n3  10407800 2021-01-01 00:01:40    65901\n4  14593381 2021-01-01 00:02:06    65901\n5  14783072 2021-01-01 00:02:09    65901\n6  14853091 2021-01-01 00:02:17    65901\n7  14920717 2021-01-01 00:03:01    65902\n8  11645517 2021-01-01 00:03:20    65902\n9  10197813 2021-01-01 00:04:32    65903\n10 14694500 2021-01-01 00:05:43    65906\n11  6335637 2021-01-01 00:05:46    65906\n12  2242096 2021-01-01 00:05:47    65906\n13  9574155 2021-01-01 00:06:09    65908\n14  6281777 2021-01-01 00:06:15    65908\n15 14260231 2021-01-01 00:07:02    65908\n16 14920186 2021-01-01 00:07:03    65908\n17 13103324 2021-01-01 00:08:01    65909\n18  1127065 2021-01-01 00:09:37    65918\n19 10841085 2021-01-01 00:10:40    65919\n20  7336289 2021-01-01 00:11:05    65920\n21 14634129 2021-01-01 00:11:17    65920\n22 14920707 2021-01-01 00:12:16    65924\n23 14461250 2021-01-01 00:13:16    65927\n24 14920741 2021-01-01 00:13:23    65927\n25 11035194 2021-01-01 00:16:10    65934\n26   735332 2021-01-01 00:17:34    65940\n27 10707986 2021-01-01 00:19:19    65944\n28 12743240 2021-01-01 00:20:09    65944\n29  1098815 2021-01-01 00:21:35    65950\n30 14496928 2021-01-01 00:21:42    65951\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nUse a window function to compute the average viewcount for each ownerid for the 10 questions preceding each question.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nFor each question, get the answer given by the user with the maximum reputation amongst users answering the question.\nHint: you’ll need to first create a subquery that determines the maximum reputation amongst the answers for each question and then use that to get the answer of interest for each question.\n\n\n\n\n\n\n\n\nChallenge (hard)\n\n\n\nFind the users who have asked one question that is highly-viewed (viewcount &gt; 1000) with their remaining questions not highly-viewed (viewcount &lt; 20 for all other questions).\n\n\n\n\n3.4 Putting it all together to do complicated queries\nHere are some real-world style questions one might try to create queries to answer. The context for these questions is a situation in which you have data on user sessions on a website or data on messages between users.\n\nGiven a table of user sessions with the format\ndate | session_id | user_id | session_time\ncalculate the distribution of the average daily total session time in the last month. I.e., you want to get each user’s daily average and then find the distribution over users. The output should be something like:\nminutes_per_day | number_of_users\nConsider a table of messages of the form\nsender_id | receiver_id | message_id\nFor each user, find the three users they message the most.\nSuppose you have are running an online experiment and have a table on the experimental design:\nuser_id | test_group | date_first_exposed\nSuppose you also have a messages table that indicates if each message was sent on web or mobile:\ndate | sender_id | receiver_id | message_id | interface (web or mobile)\nWhat is the average (over users) in the average number of messages sent per day for each test group if you look at the users who have sent messages only on mobile in the last month.\n\n\nChallenge questions with the Stack Overflow data\nIf we take the three challenges above and translate into problems for the Stack Overflow data, one could consider the following three problems.\n\nFor each user who has asked at least one question find the average number of questions per month. Then determine the distribution of that average across the users. (I.e., determine the values that would go into a histogram of the average number of questions per month across users.) The output should be something like:\nnumber of questions per month (rounded) | number of users\nNext try to figure out how to include the users who have asked no questions. Hint: think about how to get NULL values included and then count a column containing such values.\nFor each user, find the three most common tags they apply to their questions.\nThe output should be something like:\nuserid | tag | count\nHints: You’ll need to use subqueries and the final selection of just the top 3 tags will need to be done in its own query and not as part of defining a field using a window function.\nConsider trying to determine whether users who answer a lot of questions also ask a lot of questions. Grouping the users based on the number of questions they’ve answered (0, 1, 2, etc.), determine the aerage number of questions per month for each group.\nThe output should be something like:\nnumber of answers | average number of questions per month\nYou’ll want to work through this in pieces. Try to think about the initial tables you would need and then build up your query in a nested fashion.\n\n\n\n\n3.5 A summary of SQL syntax by example\nThis section shows the syntax for various queries so as to demonstrate syntax by example. It may be useful to test your understanding by figuring out (either with or without running the query) what the query does.\n\nSelecting columns\nselect ownerid, title from questions\nselect ownerid, title from questions limit 5\nselect * from questions\nselect * from questions order by answercount desc\nselect count(*) as n from questions\nselect count(ownerid) as n from questions\nselect sum(answercount) from questions\n\n\nUsing distinct\nselect distinct tag from questions_tags limit 15\nselect distinct ownerid, answercount from questions limit 15\nselect count(distinct tag) from questions_tags limit 15\n\n\nFiltering rows with where\nselect * from questions where answercount &gt; 40\nselect * from questions where answercount &gt; 40 order by answercount desc\nselect * from questions where answercount = 10 limit 5\nselect * from questions_tags where tag like 'r-%' limit 10\nselect * from questions_tags where tag similar to 'r-%|%-r|r|%-r-%' limit 10\nselect * from questions_tags where tag in ('java','r','python') limit 10\n\n\nGrouping and reduction/aggregation\nselect tag, count(*) as n from questions_tags \\\n    group by tag\n\nselect tag, count(*) as n from questions_tags \\\n    group by tag having n &gt; 1000\n    \nselect ownerid, count(*) as n from questions \\\n    group by ownerid order by n desc limit 15\n    \nselect ownerid, sum(viewcount) as viewed from questions \\\n    group by ownerid\n\nselect *, sum(viewcount) as viewed from questions \\\n    group by ownerid\n\nselect answercount, commentcount, count(*) as n from questions \\\n    group by answercount, commentcount\n\nselect tag, count(*) as n from questions_tags \\\n    where tag like 'python%' group by tag having n &gt; 1000\nThe query above starting with select *, sum(viewcount) behaves differently in SQLite and DuckDB.\n\n\nJoins\nInner joins\nselect * from questions join questions_tags \\\n    on questions.questionid = questions_tags.questionid\n    \nselect * from questions Q join questions_tags T \\\n    on Q.questionid = T.questionid\n    \nselect * from questions Q join questions_tags T \\\n    using(questionid)\n    \nselect * from questions Q, questions_tags T \\\n    where Q.questionid = T.questionid\nOuter joins\nselect * from questions Q left outer join answers A \\\n    on Q.questionid = A.questionid \n    \nselect * from questions Q left outer join answers A \\\n    on Q.questionid = A.questionid \\\n    where A.creationdate is NULL\n    \n# Note no right outer join in SQLite so here we reverse order of answers and questions \\\nselect * from questions Q right outer join answers A \\\n    on Q.questionid = A.questionid \\\n    where Q.creationdate is NULL\n\nselect Q.questionid, count(*) as n_tags from questions Q join questions_tags T \\\n    on Q.questionid = T.questionid \\\n    group by Q.questionid\nSelf joins\nFirst we’ll set up a view (a temporary) table that combines questions and tags for ease of illustrating ideas around self joins.\ncreate view QT as select * from questions join questions_tags using(questionid)\nIn small groups, discuss what these queries do.\nselect * from QT as QT1 join QT as QT2 \\\n    using(questionid)\n\nselect * from QT as QT1 join QT as QT2 \\\n    using(questionid) where QT1.tag &lt; QT2.tag\n    \nselect QT1.tag, QT2.tag, count(*) as n from QT as QT1 join QT as QT2 \\\n    using(questionid) where QT1.tag &lt; QT2.tag \\\n    group by QT1.tag, QT2.tag order by n desc limit 10\n\n\nselect * from QT as QT1 join QT as QT2 using(ownerid)\n\n\nSet operations\nselect ownerid from QT where tag='python' \\\n    intersect \\\n    select ownerid from QT where tag='r'\n    \nselect ownerid from QT where tag='python' \\\n    except \\\n    select ownerid from QT where tag='r'\n    \nselect ownerid from QT where tag='python' \\\n    union \\\n    select ownerid from QT where tag='r'\n\nselect userid, displayname, location from users \\\n    where location like '%United States%' \\\n    intersect \\\n    select userid, displayname, location from users \\\n    where reputation &gt; 10\n\n\nSubqueries\nselect * from \\\n    answers A \\\n    join \\\n    ( select ownerid, count(*) as n_answered from answers \\\n        group by ownerid order by n_answered desc limit 1000 ) most_responsive \\\n    on A.ownerid = most_responsive.ownerid\nselect avg(upvotes) from users \\\n    where userid in \\\n    ( select distinct ownerid from \\\n    questions join questions_tags using(questionid) \\\n    where tag = 'python' )",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "sql.html#efficient-sql-queries",
    "href": "sql.html#efficient-sql-queries",
    "title": "SQL",
    "section": "4 Efficient SQL queries",
    "text": "4 Efficient SQL queries\n\n4.1 Overview\nIn general, your DBMS should examine your query and try to implement it in the fastest way possible.\nSome tips for faster queries include:\n\nuse indexes on fields used in WHERE and JOIN clauses (see next section)\n\ntry to avoid wildcards at the start of LIKE string comparison when you have an index on the field (as this requires looking at all of the rows)\nsimilarly try to avoid using functions on indexed columns in a WHERE clause as this requires doing the calculation on all the rows in order to check the condition\n\nonly select the columns you really need\ncreate (temporary) tables to store intermediate results that you need to query repeatedly\nuse filtering (WHERE clauses) in inner statements when you have nested subqueries\nuse LIMIT as seen in the examples here if you only need some of the rows a query returns\n\n\n\n4.2 Indexes\nAn index is an ordering of rows based on one or more fields. DBMS use indexes to look up values quickly, either when filtering (if the index is involved in the WHERE condition) or when doing joins (if the index is involved in the JOIN condition). So in general you want your tables to have indexes.\nDBMS use indexing to provide sub-linear time lookup. Without indexes, a database needs to scan through every row sequentially, which is called linear time lookup – if there are n rows, the lookup is \\(O(n)\\) in computational cost. With indexes, lookup may be logarithmic – O(log(n)) – (if using tree-based indexes) or constant time – O(1) – (if using hash-based indexes). A binary tree-based search is logarithmic; at each step through the tree you can eliminate half of the possibilities.\nHere’s how we create an index, with some time comparison for a simple query.\n\nsystem.time(dbGetQuery(db, \"select * from questions where viewcount &gt; 10000\"))  # 2.4 seconds\nsystem.time(dbExecute(db, \"create index count_index on questions (viewcount)\")) # 5.6 seconds\nsystem.time(dbGetQuery(db, \"select * from questions where viewcount &gt; 10000\"))   # 0.9 seconds\n## restore earlier state by removing index\nsystem.time(dbExecute(db, \"drop index count_index\"))\n\nIn many contexts (but not the example above), an index can save huge amounts of time. So if you’re working with a database and speed is important, check to see if there are indexes.\nOne downside of indexes is that creation of indexes can be very time-consuming, as seen above. And if the database is updated frequently, this could be detrimental.\nFinally, using indexes in a lookup is not always advantageous, as discussed next.\n\n4.2.1 Index lookup vs. sequential scan\nUsing an index is good in that can go to the data needed very quickly based on random access to the disk locations of the data of interest, but if it requires the computer to examine a large number of rows, it may not be better than sequential scan. An advantage of sequential scan is that it will make good use of the CPU cache, reading chunks of data and then accessing the individual pieces of data quickly.\nFor example, if you compare the change the query above that filters on viewcount to use a much smaller threshold than 10000, you will probably see that the time used when there is an index is more than without an index.\nIdeally you’d do sequential scan of exactly the subset of the rows that you need, with that subset available in contiguous storage.\n\n\n4.2.2 How indexes work\nIndexes are often implemented using tree-based methods. For example in Postgres, b-tree indexes are used for indexes on things that have an ordering. Trees are basically like decision trees - at each node in the tree, there is a condition that sends one down the left or right branch (there might also be more than two branches. Eventually, one reaches the leaves of the tree, which have the actual values that one is looking for. Associated with each value is the address of where that row of data is stored. With a tree-based index, the time cost of b-tree lookup is logarithmic (based on the binary lookup), so it does grow with the number of elements in the table, but it does so slowly. The lookup process is that given a value (which would often be referred to as a key), one walks down the tree based on comparing the value to the condition at each split in the tree until one finds the elements corresponding to the value and then getting the addresses for where the desired rows are stored.\nHere’s some information on how such trees are constructed and searched.\nIn SQLite, indexes are implemented by creating a separate index table that maps from the value to the row index in the indexed table, allowing for fast lookup of a row.\n\n\n\n4.3 SQL query plans and EXPLAIN\nYou can actually examine the query plan that the system is going to use for a query using the EXPLAIN keyword. I’d suggest trying this in Postgres as the output is more interpretable than SQLite.\n\ndbGetQuery(db, \"explain select * from webtraffic where count &gt; 500\")\n\nIn PostgreSQL that gives the following:\n                                                                        QUERY PLAN\n1                             Gather  (cost=1000.00..388634.17 rows=8513 width=61)\n2                                                               Workers Planned: 2\n3   -&gt;  Parallel Seq Scan on webtraffic  (cost=0.00..386782.88 rows=3547 width=61)\n4                                                            Filter: (count &gt; 500)\nThe “Workers Planned: 2” seems to indicate that there will be some parallelization used, even without us asking for that.\nNow let’s see what query plan is involved in a join and when using indexes.\n\ndbGetQuery(db, \"explain select * from questions join questions_tags on\n               questions.questionid = questions_tags.questionid\")\n\n                                                                         QUERY PLAN\n1                   Hash Join  (cost=744893.91..2085537.32 rows=39985376 width=118)\n2                     Hash Cond: (questions_tags.questionid = questions.questionid)\n3     -&gt;  Seq Scan on questions_tags  (cost=0.00..634684.76 rows=39985376 width=16)\n4                     -&gt;  Hash  (cost=365970.96..365970.96 rows=13472796 width=102)\n5         -&gt;  Seq Scan on questions  (cost=0.00..365970.96 rows=13472796 width=102)\n\ndbGetQuery(db, \"explain select * from questions join questions_tags on\n               questions.questionid = questions_tags.questionid where tag like 'python'\")\n\n                                                                                                QUERY PLAN\n1                                                 Gather  (cost=15339.05..899172.92 rows=687748 width=118)\n2                                                                                       Workers Planned: 2\n3                                        -&gt;  Nested Loop  (cost=14339.05..829398.12 rows=286562 width=118)\n4         -&gt;  Parallel Bitmap Heap Scan on questions_tags  (cost=14338.61..252751.63 rows=286562 width=16)\n5                                                                          Filter: (tag ~~ 'python'::text)\n6               -&gt;  Bitmap Index Scan on questions_tags_tag_idx  (cost=0.00..14166.68 rows=687748 width=0)\n7                                                                       Index Cond: (tag = 'python'::text)\n8                     -&gt;  Index Scan using questions_pkey on questions  (cost=0.43..2.01 rows=1 width=102)\n9                                                     Index Cond: (questionid = questions_tags.questionid)\nHere’s additional information on interpreting what you see: https://www.postgresql.org/docs/current/static/using-explain.html.\nThe main thing to look for is to see if the query will be done by using an index or by sequential scan (i.e., looking at all the rows).\nFinally, let’s compare the query plans for an inner join versus a cross join followed by a WHERE that produces equivalent results.\n\ndbGetQuery(db, \"explain select * from questions join questions_tags on\n               questions.questionid = questions_tags.questionid\")\n\n                                                                         QUERY PLAN\n1                   Hash Join  (cost=744893.91..2085537.32 rows=39985376 width=118)\n2                     Hash Cond: (questions_tags.questionid = questions.questionid)\n3     -&gt;  Seq Scan on questions_tags  (cost=0.00..634684.76 rows=39985376 width=16)\n4                     -&gt;  Hash  (cost=365970.96..365970.96 rows=13472796 width=102)\n5         -&gt;  Seq Scan on questions  (cost=0.00..365970.96 rows=13472796 width=102)\n6                                                                              JIT:\n7                                                                     Functions: 10\n8       Options: Inlining true, Optimization true, Expressions true, Deforming true\n\ndbGetQuery(db, \"explain select * from questions cross join questions_tags where\n               questions.questionid = questions_tags.questionid\")\n\n                                                                         QUERY PLAN\n1                   Hash Join  (cost=744893.91..2085537.32 rows=39985376 width=118)\n2                     Hash Cond: (questions_tags.questionid = questions.questionid)\n3     -&gt;  Seq Scan on questions_tags  (cost=0.00..634684.76 rows=39985376 width=16)\n4                     -&gt;  Hash  (cost=365970.96..365970.96 rows=13472796 width=102)\n5         -&gt;  Seq Scan on questions  (cost=0.00..365970.96 rows=13472796 width=102)\n6                                                                              JIT:\n7                                                                     Functions: 10\n8       Options: Inlining true, Optimization true, Expressions true, Deforming true\nWe see that the query plan indicates the two queries are using the same steps, with the same cost.\n\n\n4.4 Disk caching\nYou might think that database queries will generally be slow (and slower than in-memory manipulation such as in R or Python when all the data can fit in memory) because the database stores the data on disk. However, as mentioned on the introduction page, the operating system will generally cache files/data in memory when it reads from disk. Then if that information is still in memory the next time it is needed, it will be much faster to access it the second time around. Other processes might need memory and ‘invalidate’ the cache, but often once the data is read once, the database will be able to do queries quite quickly. This also means that even if you’re using a database, you can benefit from a machine with a lot of memory if you have a large database (ideally a machine with rather more RAM than the size of the table(s) you’ll be accessing).\nGiven this, it generally won’t be helpful to force your database to reside in memory (e.g., using :memory: for SQLite or putting the database on a RAM disk).\n\n\n4.5 Parallelization and partitioning\nTo speed up your work, one might try to split up one’s queries into multiple queries that you run in parallel. However, you’re likely to have problems with parallel queries from a single R or Python session.\nHowever, multiple queries to the same database from separate R or Python sessions will generally run fine but can compete for access to disk/memory. That said, in some basic experiments, the slowdown was moderate, so one may be able to parallelize across processes in a manual fashion.\nAs of version 9.6 of Postgres, there is some capability for doing parallel queries: https://www.postgresql.org/docs/current/static/parallel-query.html.\nFinally Postgres supports partitioning tables. Generally one would divide a large table into smaller tables based on unique values of a key. For example if your data had timetamps, you could partition into subtables for each month or each year. This would allow faster queries when considering data that reside on one or a small number of partitions and could also ease manual implementation of parallelization. Here’s some information: https://www.postgresql.org/docs/current/static/ddl-partitioning.html.\n\n\n4.6 DuckDB\nFor a serverless database, DuckDB is a nice alternative to SQLite that may speed up queries substantially. DuckDB stores data column-wise, which can lead to big speedups when doing queries operating on large portions of tables (so-called “online analytical processing” (OLAP)). Also, in this case, working with a column-wise format may faster than using an index.\nLet’s compare timing using DuckDB and SQLite versions of the StackOverflow database.\nFirst, we’ll see that simple queries that have to process an entire column can be much faster in DuckDB.\n\nlibrary(duckdb)\ndrv &lt;- duckdb()\ndbFilename &lt;- 'stackoverflow-2021.duckdb'\ndbDuck &lt;- dbConnect(drv, file.path(dir, dbFilename))\n\nsystem.time(dbGetQuery(db, \"select count(ownerid) from questions\"))\n\n   user  system elapsed \n  0.169   0.157   1.551 \n\nsystem.time(dbGetQuery(dbDuck, \"select count(ownerid) from questions\"))\n\n   user  system elapsed \n  0.034   0.004   0.078 \n\nsystem.time(result1 &lt;- dbGetQuery(db, \"select distinct ownerid from questions\"))\n\n   user  system elapsed \n  1.900   1.266   3.331 \n\nsystem.time(result2 &lt;- dbGetQuery(dbDuck, \"select distinct ownerid from questions\"))\n\n   user  system elapsed \n  0.207   0.020   0.038 \n\n\nNow let’s compare timings for some of the queries we ran previously. There are substantial speed-ups in both cases.\nHere’s a simple join with a filter.\n\nsystem.time(result1 &lt;- dbGetQuery(db, \"select * from questions join questions_tags \n                           on questions.questionid = questions_tags.questionid \n                           where tag = 'python'\"))\n\n   user  system elapsed \n  3.474   0.872   4.770 \n\nsystem.time(result2 &lt;- dbGetQuery(dbDuck, \"select * from questions join questions_tags \n                           on questions.questionid = questions_tags.questionid \n                           where tag = 'python'\"))\n\n   user  system elapsed \n  0.881   0.126   1.180 \n\n\nAnd here’s a subquery in the FROM statement.\n\nsystem.time(result1 &lt;- dbGetQuery(db, \"select * from questions join answers A\n                on questions.questionid = A.questionid\n                join\n                (select ownerid, count(*) as n_answered from answers\n                group by ownerid order by n_answered desc limit 1000) most_responsive\n                on A.ownerid = most_responsive.ownerid\"))\n\n   user  system elapsed \n  8.130   1.862  10.237 \n\nsystem.time(result2 &lt;- dbGetQuery(dbDuck, \"select * from questions join answers A\n                on questions.questionid = A.questionid\n                join\n                (select ownerid, count(*) as n_answered from answers\n                group by ownerid order by n_answered desc limit 1000) most_responsive\n                on A.ownerid = most_responsive.ownerid\"))\n\n   user  system elapsed \n  2.310   0.125   1.484 \n\n\nDuckDB will run in parallel by using multiple threads, which can help speed up computations on top of efficiencies available through the column-wise storage, though with the speed of the DuckDB queries above, I don’t think parallelization made much difference in those cases.\nYou can manage the number of threads like this:\n\ndbExecute(dbDuck, \"set threads to 4\")\n\n[1] 0\n\ndbGetQuery(dbDuck, \"SELECT current_setting('threads')\")\n\n  current_setting('threads')\n1                          4\n\ndbDisconnect(dbDuck, shutdown = TRUE)\n\nFinally, DuckDB databases tend to take up less space on disk than SQLite databases, because the column-wise storage allows for better compression.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "bigquery.html",
    "href": "bigquery.html",
    "title": "Data Warehouses",
    "section": "",
    "text": "Data warehouses such as Google BigQuery and Amazon RedShift allow you to create a data repository in which the data are structured like in a database (tables, fields, etc.), stored in the cloud, and queried efficiently (and in parallel) using the cloud provider’s infrastructure. Storage is by column, which allows for efficient queries when doing queries operating on large portions of tables.\nBigQuery has some nice advantages over RedShift for one-off analyses (as opposed to regular, repeated operations). In particular, it requires less configuration/administration. BigQuery is ‘serverless’. Instead of you having to set up virtual machines to do the queries, BigQuery automatically sets up the computational resources for your query, scaled as appropriate for the size of the query.\nSome features of BigQuery include:\n\nYou can manage access to your datasets for different GCP users.\nYou can query publicly available datasets.\nYou can query “external tables” to access data not in a BigQuery dataset.\n\n\nThe instructions here were prepared in 2023 and the interface may have changed since then.",
    "crumbs": [
      "Databases in the Cloud"
    ]
  },
  {
    "objectID": "bigquery.html#overview",
    "href": "bigquery.html#overview",
    "title": "Data Warehouses",
    "section": "",
    "text": "Data warehouses such as Google BigQuery and Amazon RedShift allow you to create a data repository in which the data are structured like in a database (tables, fields, etc.), stored in the cloud, and queried efficiently (and in parallel) using the cloud provider’s infrastructure. Storage is by column, which allows for efficient queries when doing queries operating on large portions of tables.\nBigQuery has some nice advantages over RedShift for one-off analyses (as opposed to regular, repeated operations). In particular, it requires less configuration/administration. BigQuery is ‘serverless’. Instead of you having to set up virtual machines to do the queries, BigQuery automatically sets up the computational resources for your query, scaled as appropriate for the size of the query.\nSome features of BigQuery include:\n\nYou can manage access to your datasets for different GCP users.\nYou can query publicly available datasets.\nYou can query “external tables” to access data not in a BigQuery dataset.\n\n\nThe instructions here were prepared in 2023 and the interface may have changed since then.",
    "crumbs": [
      "Databases in the Cloud"
    ]
  },
  {
    "objectID": "bigquery.html#running-queries",
    "href": "bigquery.html#running-queries",
    "title": "Data Warehouses",
    "section": "2 Running queries",
    "text": "2 Running queries\nLet’s first demonstrate running an SQL query in BigQuery. We’ll use a publicly available dataset. You can process your first one terabyte of data per month for free (that sounds like a lot, but queries can add up quickly when datasets are gigatyes in size or larger).\n\n2.1 Using the BigQuery browser-based interface\nThese next instructions assume you have a Google Cloud Platform account with billing set up (e.g., using a credit card). To use the BigQuery Sandbox, see below.\nNavigate to cloud.google.com and login (or create if needed) your Google Cloud Platform account (which will be associated with your Google account and its email address). Then go to the BiqQuery website and click Enable (if you haven’t enabled it previously for your account).\nNow we can start working with BigQuery. The demo here follows these instructions.\n\n\n\nBigQuery interface, showing the welcome page.\n\n\nTo use a public dataset, we’ll choose Add Data and scroll down to Public Datasets. Search for “Stack Overflow”. Click on View Dataset. You’ll be able to see that the name of the dataset is bigquery-public-data.stackoverflow. This dataset has similar data to the Stack Overflow database used elsewhere in this tutorial, but the schema differs somewhat.\nIn the Explorer pane you can search for “Stack Overflow” and be able to view the tables, and then select tables to see the fields.\n\n\n\nBigQuery interface, showing the SQL Explorer pane highlight the public stackoverflow dataset.\n\n\nNow we can go to the Editor pane or select `Compose a New Query’ and enter an SQL query.\n\n\n\nBigQuery interface, showing the SQL Editor pane.\n\n\nNote that BigQuery will preview how much data will be processed (see the upper right). Remember that you can process 1 TB for free each month.\nThis example query would process 37 GB. Maybe more of our free quota than we want to use at the moment.\n\nselect * from `bigquery-public-data.stackoverflow.posts_questions` limit 5;\n\nThis will only process 2.5 MB, so let’s run this:\n\nselect * from `bigquery-public-data.stackoverflow.tags` order by count desc limit 20;\n\nThe result (which you can see in the next section run from R) indicates that javascript, python, java, and c# are the most popular tags.\nYou can save the results in various ways (to a local file, to Google Drive, as a BigQuery table, etc.\n\n2.1.1 Using the free BigQuery Sandbox\nYou can use the Sandbox to avoid having to set up billing for your GCP account. The Sandbox has the same limits as BigQuery’s free tier, in particular 1 TB of processed query data each month.\nGo to BigQuery, and login with your Google account. Set up a new project and then go to BigQuery and try it out (e.g., running the query above on the StackOverflow data).\n\n\n\n2.2 Running queries from R or Python\n\n2.2.1 Using R\nWe can use the bigrquery R package to interact with BigQuery from R (e.g., running on a laptop).\n\nmyproject &lt;- \"scf-gvm0\"\nuser &lt;- \"paciorek@berkeley.edu\"\n\nlibrary(bigrquery)\nlibrary(DBI)\n\nThe following will prompt for authentication to your Google account in a browser and save the authentication information as a ‘token’ associated with the email address.\nIn principle, you shouldn’t have to do this explicitly, but rather the first time you try to interact with a BigQuery dataset, bigrquery should prompt you to authenticate, or if you’ve previously authenticated, it will reload a saved token. But I’ve had some trouble getting this to work without explicitly running this:\n\ntoken &lt;- gargle::credentials_user_oauth2(scopes = \"https://www.googleapis.com/auth/bigquery\", email = user)\nbigrquery::bq_auth(token = token)\n\nNow let’s connect to the public dataset and check the schema. To be able to run queries, we need to be able to provide a GCP project that has billing set up via the billing argument. It’s possible that the SQL below would run if billing is set to be the name of a project without billing set up (i.e., using the BigQuery Sandbox), but I haven’t checked that.\n\ndrv &lt;- bigquery()\n\ndb &lt;- dbConnect(drv,\n  project = \"bigquery-public-data\",\n  dataset = \"stackoverflow\",\n  billing = myproject\n)\n\ndbListTables(db)\n\n [1] \"badges\"                     \"comments\"                  \n [3] \"post_history\"               \"post_links\"                \n [5] \"posts_answers\"              \"posts_moderator_nomination\"\n [7] \"posts_orphaned_tag_wiki\"    \"posts_privilege_wiki\"      \n [9] \"posts_questions\"            \"posts_tag_wiki\"            \n[11] \"posts_tag_wiki_excerpt\"     \"posts_wiki_placeholder\"    \n[13] \"stackoverflow_posts\"        \"tags\"                      \n[15] \"users\"                      \"votes\"                     \n\ndbListFields(db, 'posts_questions')\n\n [1] \"id\"                       \"title\"                   \n [3] \"body\"                     \"accepted_answer_id\"      \n [5] \"answer_count\"             \"comment_count\"           \n [7] \"community_owned_date\"     \"creation_date\"           \n [9] \"favorite_count\"           \"last_activity_date\"      \n[11] \"last_edit_date\"           \"last_editor_display_name\"\n[13] \"last_editor_user_id\"      \"owner_display_name\"      \n[15] \"owner_user_id\"            \"parent_id\"               \n[17] \"post_type_id\"             \"score\"                   \n[19] \"tags\"                     \"view_count\"              \n\n\nNow let’s run the simple query we tried before.\n\nsql &lt;- \"select * from tags order by count desc limit 20\"\ndbGetQuery(db, sql)\n\n# A tibble: 20 × 5\n      id tag_name     count excerpt_post_id wiki_post_id\n   &lt;int&gt; &lt;chr&gt;        &lt;int&gt;           &lt;int&gt;        &lt;int&gt;\n 1     3 javascript 2426650         3624960      3607052\n 2    16 python     2026741         3624965      3607014\n 3    17 java       1866104         3624966      3607018\n 4     9 c#         1559412         3624962      3607007\n 5     5 php        1445747         3624936      3607050\n 6  1386 android    1386459         3625001      3607484\n 7     2 html       1146245         3673183      3673182\n 8   820 jquery     1029561         3625262      3607053\n 9    10 c++         776837         3624963      3606997\n10     4 css         771867         3644670      3644669\n11 58338 ios         674982         4536664      4536663\n12    21 mysql       651413         3624969      3607033\n13    22 sql         643145         3625226      3607304\n14  4452 r           464242         3625322      3607736\n15 46426 node.js     442811         4238969      4238968\n16 92497 reactjs     415837        16880335     16880334\n17   114 arrays      398827         4969094      4969093\n18     8 c           385511         3624961      3607013\n19    96 asp.net     370124         3625232      3607037\n20  1508 json        346406         4889848      4889847\n\n\n\n\n2.2.2 Using Python\nWe can use the Google BigQuery Python client (package) provided by Google. (We’ll also install the db-dtypes package so that we convert the result of our query into a Pandas data frame.)\n\npip install google-cloud-bigquery db-dtypes\n\nTo authenticate, you’ll need to use the gcloud command line interface. After installing that, we run the authentication command.\n\ngcloud auth application-default login\n\nThis will bring up a browser window and you can authenticate with your Google account. It will save your credentials to a file in your home directory, e.g., to ~/.config/gcloud/application_default_credentials.json. These credentials will then be used by Google Cloud clients such as the BigQuery client.\n\nfrom google.cloud import bigquery\nimport db_dtypes  # needed for `to_dataframe()`\n\nmyproject &lt;- \"some_project\"\ndb = bigquery.Client(project = myproject)\n\nsql = \"select * from bigquery-public-data.stackoverflow.tags order by count desc limit 20\"\nquery_job = db.query(sql)  # API request\nrows = query_job.result()  # Waits for query to finish\n\nmydf = rows.to_dataframe()\nmydf\n\nThe results (not shown) are the same as when done from R.",
    "crumbs": [
      "Databases in the Cloud"
    ]
  },
  {
    "objectID": "bigquery.html#getting-data-into-bigquery",
    "href": "bigquery.html#getting-data-into-bigquery",
    "title": "Data Warehouses",
    "section": "3 Getting data into BigQuery",
    "text": "3 Getting data into BigQuery\nNext, let’s demonstrate setting up a BigQuery data warehouse, using the Wikipedia data (used in the database management demos) as an example. Recall that we have data to populate a single table from multiple space-delimited flat text files, all of them in exactly the same format in terms of columns.\n\n3.1 Loading a local file\nWe can load from a local file on our computer using Add Data, or we can load from files already present in Google Cloud Storage.\nFirst we’ll need to set IAM permissions on our account to allow the account to create and update tables.\nTo upload from a local file, select the Add Data button and choose the file. There is no ‘space-separated’ file format, so we’ll choose CSV and then in the advanced options, I need to specify that the delimiter is a space using a “custom” field delimiter. You can choose to create a new dataset. Since the files don’t have column names, I’ll choose to specify the schema (the field names and types) in the Schema section of the BigQuery form by clicking on the ‘+’ symbol to add each field, specifically by entering this information: {date: string, maxlength = 8, hour: string: maxlength = 6, site: string, page: string, count: integer, size: numeric}.\n\n\n\nBigQuery interface, showing the SQL Editor pane.\n\n\nI’ll name the new BigQuery dataset wiki_test (this is equivalent to the name of a database in a standard DBMS).\nAs was the case in the data management demos, we need to strip out double quote (“) symbols as BigQuery tries to interpret these as beginning and ending fields.\nIn the Add Data form, it looks like we can only add a single file at a time, so for this test, I’ll just add one file.\nYou can then test things worked by trying to query the new table in the new dataset from R as done previously with the public Stack Overflow dataset. In an earlier version of this tutorial in 2023, I had this working, but I haven’t gotten this working again after some changes to my GCP account, so here we’ll just see what one should get if it’s all set up.\n\ndb&lt;- dbConnect(drv,\n  project = myproject,\n  dataset = \"wiki_test\",\n  billing = myproject\n)\n\n\ndbListTables(db)\ndbListFields(db, 'webtraffic')\nsql &lt;- \"select * from webtraffic limit 5\"\ndbGetQuery(db, sql)\n\nIf it works you should see results like this:\n## [1] \"webtraffic\"\n## [1] \"date\"  \"hour\"  \"site\"  \"page\"  \"count\" \"size\"\n## # A tibble: 5 × 6\n##   date     hour   site  page                        count   size\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                       &lt;int&gt;  &lt;dbl&gt;\n## 1 20081101 000001 af    Halloween                      12  94174\n## 2 20081101 000001 af    Spesiaal:Onlangse_wysigings    25 360372\n## 3 20081101 000001 af    Spesiaal:Search                50  52332\n## 4 20081101 000001 af    Tempelhof-lughawe              26 115063\n## 5 20081101 000001 af    Tuisblad                       33 514829\n\n\n3.2 Loading from Google Cloud Storage\nNow let’s load multiple files all at once from Google Cloud Storage (GCS).\nFirst we’ll take 10 of the files of the cleaned (no double quotes) Wikipedia data and put them in GCS. Through the GCS web interface, I’ll create a bucket named wikipedia-cleaned-data and upload the files.\nNow select Add Data in the BigQuery interface. We’ll select the source as Google Cloud Storage, the file format as CSV (with the custom separator of a space character) and use a “URI pattern” to select all 10 of the files: wikipedia-cleaned-data/part-*. We’ll name this dataset wikipedia.\nWe can now run the same query as above on the new (larger) dataset:\n\ndb&lt;- dbConnect(drv,\n  project = myproject,\n  dataset = \"wikipedia\",\n  billing = myproject\n)\n\n\ndbListTables(db)\ndbListFields(db, 'webtraffic')\nsql &lt;- \"select * from webtraffic limit 5\"\ndbGetQuery(db, sql)\n\nAgain, if it worked, you should see results like:\n## [1] \"webtraffic\"\n## [1] \"date\"  \"hour\"  \"site\"  \"page\"  \"count\" \"size\"\n## # A tibble: 5 × 6\n##   date     hour   site  page         count    size\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;int&gt;   &lt;dbl&gt;\n## 1 20081101 000001 fr    Psychose        15  295730\n## 2 20081101 000001 fr    Skyrock_Blog    10  192848\n## 3 20081101 000001 fr    Tunisie         36 3736262\n## 4 20081101 000001 fr    Ulc%C3%A8re     10   60423\n## 5 20081101 000001 fr    X_Japan         11  258634\n\n\n3.3 Permissions\nI did this using my Google Cloud account, which is an owner of the GCP account and so has various BigQuery permissions already set to allow various actions, including creating or modifying BigQuery datasets.\nIf you (as recommended) are using an IAM account, you are likely to need to set some permissions for the account to create or modify BigQuery datasets. In particular, as discussed here you need the following IAM permissions set:\n\nbigquery.tables.create\nbigquery.tables.updateData\nbigquery.tables.update\nbigquery.jobs.create\n\nYou can set these permissions by adding one of various IAM roles (e.g., roles/bigquery.dataEditor, roles/bigquery.dataOwner, roles/bigquery.admin, bigquery.user, or bigquery.jobUser).",
    "crumbs": [
      "Databases in the Cloud"
    ]
  },
  {
    "objectID": "R-and-Python.html",
    "href": "R-and-Python.html",
    "title": "Working with big data in R and Python",
    "section": "",
    "text": "This section aims to provide an overview of working with large datasets in R and (to a lesser extent) Python. Given the scope of topics, this is not meant to be a detailed treatment of each topic.\nWe’ll start with a refresher on data frames in R and Python and some discussion of the dplyr package, whose standard operations are similar to using SQL syntax. Note that what is referred to as split-apply-combine functionality in dplyr in R and in pandas in Python is the same concept as the use of SQL’s GROUP BY combined with aggregation operations such as MIN, MAX, AVG, COUNT.\nThe CSV files for the 2016 Stack Overflow data and the space-delimited files for the Wikipedia traffic data used in the examples below can be obtained here.",
    "crumbs": [
      "Big Data in R and Python"
    ]
  },
  {
    "objectID": "R-and-Python.html#data-frames-in-r-and-python",
    "href": "R-and-Python.html#data-frames-in-r-and-python",
    "title": "Working with big data in R and Python",
    "section": "1 Data frames in R and Python",
    "text": "1 Data frames in R and Python\n\n1.1 Data frames in R\nA data frame in R is essentially the same as a table in SQL. The notion of a data frame has been essential to the success of R and its existence inspired Python’s Pandas package.\nR’s data frames are stored in memory, but there are now packages (such as dplyr with an SQL backend, arrow, SparkR and h2o) that allow you to treat an external data source as if it were an actual R data frame, using familiar syntax to operate on the data frame.\nThis tutorial assumes you’re familiar with basic data frame functionality in R or Python, so I won’t go into more details here.\ndplyr, which will be discussed later, allows you to operate on data frames using functionality that is similar to SQL, in particular selecting columns, filtering rows, aggregation operations on subsets, and joining multiple data frames.\nBut base R syntax can be used for all of these operations too. Here’s the base R syntax corresponding to SQL’s SELECT, WHERE, GROUP BY, and JOIN functionality.\n\nusers &lt;- read.csv(file.path('data', 'users-2016.csv'))\nquestions &lt;- read.csv(file.path('data', 'questions-2016.csv'))\nusers[ , c('userid', 'upvotes')] # select columns\nusers[users$upvotes &gt; 10000, ]   # filter by row (i.e., SQL WHERE)\naggregate(upvotes ~ age, data = users, FUN = median) # group by (i.e., aggregation)\njoined &lt;- merge(users, questions, by.x = 'userid', by.y = 'ownerid',\n    all.x = FALSE, all.y = FALSE)  # inner join\n\n\n\n1.2 Using SQL syntax with R data frames: sqldf\nThe sqldf package provides the ability to use SQL queries on R data frames (via sqldf) and on-the-fly when reading from CSV files (via read.csv.sql). The latter can help you avoid reading in the entire dataset into memory in R if you just need a subset of it.\nThe basic sequence of operations that happens is that the data frame (if using sqldf) or the file (if using read.csv.sql) is read temporarily into a database and then the requested query is performed on the database, returning the result as a regular R data frame. So you might find things to be a bit slow because of the time involved in creating the database.\nThe following illustrates usage but the read.csv.sql part of the code won’t work in practice on this particular example input file, because sqldf regards quotes as part of the text and not as delineating fields. The CSVs for the Stack Overflow data all have quotes distinguishing fields because there are commas within some fields.\n\nlibrary(sqldf)\n## sqldf\nusers &lt;- read.csv(file.path('data','users-2016.csv'))\noldUsers &lt;- sqldf(\"select * from users where age &gt; 75\")\n\n## read.csv.sql with data read into an in-memory database\noldUsers &lt;- read.csv.sql(file.path('data', 'users-2016.csv'),  \n      sql = \"select * from file where age &gt; 75\",\n      dbname = NULL, header = TRUE)\n## read.csv.sql with data read into temporary database on disk\noldUsers &lt;- read.csv.sql(file.path('data', 'users-2016.csv'),  \n      sql = \"select * from file where age &gt; 75\",\n      dbname = tempfile(), header = TRUE)\n\nAnd note that one can use sqldf::read.csv.sql to avoid reading all the data in from disk.\n\n\n1.3 Data frames in Python\nThe Pandas package has nice functionality for doing dataset manipulations akin to SQL queries including group by/aggregation operations, using a data structure called a DataFrame inspired by R’s data frames. Furthermore, Pandas was designed from the start for computational efficiency, in contrast to standard data frames in R (but see below for newer R functionality that is much more efficient).\nHere are some examples:\n\nimport pandas as pd\nimport os\nusers = pd.read_csv(os.path.join('data', 'users-2016.csv'))\nquestions = pd.read_csv(os.path.join('data', 'questions-2016.csv'))\ntype(users)\nusers[['userid', 'upvotes']]   # select columns         \nusers[users.upvotes &gt; 10000]   # filter by row (i.e., sql WHERE)\n# group by (i.e., aggregation)\nusers.groupby('age')['upvotes'].agg({'med': 'median', 'avg': 'mean'}) \njoined = pd.merge(users, questions, how= 'inner', left_on= 'userid',\n        right_on = 'ownerid')\n\nPolars is a newer dataframe package that provides a Python interface and is designed to be fast.\nHere are some examples:\n\nimport polars as pl\nimport os\nusers = pl.read_csv(os.path.join('data', 'users-2016.csv'))\ntags = pl.read_csv(os.path.join('data', 'questions_tags-2016.csv'))\nquestions = pl.read_csv(os.path.join('data', 'questions-2016.csv'))\ntype(users)\nusers.select(pl.col('userid','upvotes'))   # select columns         \nusers.filter(pl.col('upvotes') &gt; 10000)    # filter by row (i.e., sql WHERE)\n# group by (i.e., aggregation)\ntags.groupby('tag').agg(\n        pl.col('*').count().alias('n'))\ntags.groupby('tag').agg(\n        pl.col('*').count().alias('n')).sort('n', descending=True).head(8)\n## Users userid got read in as an integer but Questions ownerid as string.\nusers.schema\nusers = pl.read_csv(os.path.join('data', 'users-2016.csv'), dtypes = {'userid': str})\njoined = users.join(questions, how= 'inner', left_on= 'userid',\n        right_on = 'ownerid')\n\nHere’s a further example that compares Pandas and Polars, building off of this blog post comparing R’s dplyr to Pandas. As illustrated in the example and above, I think the Polars interface (API) is easier to read and use than that of Pandas.\n\n\n1.4 Distributed data frames in Dask in Python\nThe Dask package provides the ability to divide data frames across multiple workers (and across nodes), allowing one to handle very large datasets, as discussed in this tutorial.",
    "crumbs": [
      "Big Data in R and Python"
    ]
  },
  {
    "objectID": "R-and-Python.html#dplyr-in-r",
    "href": "R-and-Python.html#dplyr-in-r",
    "title": "Working with big data in R and Python",
    "section": "2 dplyr in R",
    "text": "2 dplyr in R\n\n2.1 Overview\ndplyr is part of the tidyverse, a set of R packages spearheaded by Hadley Wickham. You can think of dplyr as providing the functionality of SQL (selecting columns, filtering rows, transforming columns, aggregation, and joins) on R data frames using a clean syntax that is easier to use than base R operations.\nThere’s lots to dplyr, but here we’ll just illustrate the basic operations by analogy with SQL.\nHere we’ll read the data in and do some basic subsetting. In reading the data in we’ll use another part of the tidyverse: the readr package, which provides read_csv as a faster version of read.csv.\n\nlibrary(dplyr)\nusers &lt;- readr::read_csv(file.path('data', 'users-2016.csv'))\nresult &lt;- select(users, userid, displayname)  # select columns\ndim(result)\n\n[1] 1104795       2\n\nresult &lt;- filter(users, age &gt; 75)             # filter by row (i.e., SQL WHERE)\ndim(result)\n\n[1] 481  10\n\n\n\n\n2.2 Piping\ndplyr is often combined with piping, which allows you to build up a sequence of operations (from left to right), as if you were using UNIX pipes or reading a series of instructions. Here’s a very simple example where we combine column selection and filtering in a readable way:\n\nresult &lt;- users %&gt;% select(displayname, userid, age) %&gt;% filter(age &gt; 75)\n## Or using the new pipe operator from base R:\nresult &lt;- users |&gt; select(displayname, userid, age) |&gt; filter(age &gt; 75)\n\nWhat happens here is that the operations are run from left to right (except for the assignment into result) and the result of the left-hand side of a %&gt;% is passed into the right-hand side function as the first argument. So this one liner is equivalent to:\n\ntmp &lt;- select(users, displayname, userid, age)\nresult2 &lt;- filter(tmp, age &gt; 75)\nidentical(result, result2)\n\n[1] TRUE\n\n\nand also equivalent to:\n\nresult3 &lt;- filter(select(users, displayname, userid, age), age &gt; 75)\nidentical(result, result3)\n\n[1] TRUE\n\n\nWe’ll use pipes in the remainder of the dplyr examples.\n\n\n2.3 Functionality\nHere’s how one can do stratified analysis with aggregation operations. In the dplyr world, this is known as split-apply-combine but in the SQL world this is just a GROUP BY with some aggregation operation.\n\nmedianVotes &lt;- users %&gt;% group_by(age) %&gt;% summarize(\n                          median_upvotes = median(upvotes),\n                          median_downvotes = median(downvotes))\nhead(medianVotes)\n\n# A tibble: 6 × 3\n    age median_upvotes median_downvotes\n  &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1    13           11                  0\n2    14            0.5                0\n3    15            0                  0\n4    16            3                  0\n5    17            3                  0\n6    18            3                  0\n\n\nYou can also create new columns, sort, and do joins, as illustrated here:\n\n## create new columns\nusers2 &lt;- users %&gt;% mutate(year = substring(creationdate, 1, 4),\n                           month = substring(creationdate, 6, 7))\n## sorting (here in descending (not the default) order by upvotes)\nusers2 &lt;- users %&gt;% arrange(age, desc(upvotes))\n## joins\nquestions &lt;- readr::read_csv(file.path('data', 'questions-2016.csv'))\nquestionsOfAge &lt;- users %&gt;% filter(age &gt; 75) %&gt;%\n               inner_join(questions, by = c(\"userid\" = \"ownerid\"))\nhead(questionsOfAge)\n\n# A tibble: 6 × 15\n  userid creationdate.x      lastaccessdate      location    \n   &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;       \n1   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n2   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n3   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n4   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n5   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n6   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n# ℹ 11 more variables: reputation &lt;dbl&gt;, displayname &lt;chr&gt;,\n#   upvotes &lt;dbl&gt;, downvotes &lt;dbl&gt;, age &lt;dbl&gt;, accountid &lt;dbl&gt;,\n#   questionid &lt;dbl&gt;, creationdate.y &lt;dttm&gt;, score &lt;dbl&gt;,\n#   viewcount &lt;dbl&gt;, title &lt;chr&gt;\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nWhy did I first filter and then do the join, rather than the reverse?\n\n\nThe join functions include inner_join, left_join, right_join, full_join. I don’t see any cross join functionality.\nIn addition to operating directly on data frames, dplyr can also operate on databases and data.table objects as the back-end storage, as we’ll see next.\n\n\n2.4 Cautionary notes\nNote that dplyr and other packages in the tidyverse use a modified form of data frames. In some cases you may want to convert back to a standard data frame using as.data.frame. For example:\n\nas.data.frame(head(questionsOfAge, 3))\n\n  userid      creationdate.x      lastaccessdate     location\n1   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n2   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n3   4668 2008-09-05 04:08:05 2017-03-13 21:19:14 Portland, OR\n  reputation displayname upvotes downvotes age accountid\n1     116900  Alan Storm    2143       278  97      3253\n2     116900  Alan Storm    2143       278  97      3253\n3     116900  Alan Storm    2143       278  97      3253\n  questionid      creationdate.y score viewcount\n1   34552563 2016-01-01 00:02:22     2       484\n2   34597749 2016-01-04 18:46:36     5       250\n3   34689333 2016-01-09 03:15:07     2       119\n                                                          title\n1                           PHP: Phing, Phar, and phar.readonly\n2 Determine if PHP files is Running as Part of a `phar` archive\n3     List of PHP Keywords that are Invalid as Class Name Parts\n\n\nNote that dplyr and other tidyverse packages use a lot of “non-standard evaluation”. In this context of non-standard evaluation, the thing to pay attention to is that the column names are not quoted. This means that one cannot use a variable to stand in for a column. So the following woudn’t work because dplyr would literally look for a variable named “colname” in the data frame. There is a system for addressing this but I won’t go into it further here.\n\n## this won't work because of non-standard evaluation! \nmyfun &lt;- function(df, colname) \n  select(df, colname)\nmyfun(questions, 'age')\n\n\n\n2.5 dplyr with SQL and databases\nWe can connect to an SQLite or Postgres database and then query it using dplyr syntax:\n\nlibrary(RSQLite)\ndrv &lt;- dbDriver(\"SQLite\")\ndb &lt;- dbConnect(drv, dbname = file.path('data', 'stackoverflow-2016.db'))\nusers &lt;- tbl(db, 'users')\nhighVotes &lt;- users %&gt;% filter(upvotes &gt; 10000)\nhead(highVotes)\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.47.1 [/accounts/web/public/paciorek/share/stackoverflow-2016.db]\n  userid creationdate        lastaccessdate   location reputation\n   &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;            &lt;chr&gt;         &lt;int&gt;\n1   3043 2008-08-26 13:24:14 2017-03-13 17:0… York, NE     258471\n2   5987 2008-09-11 21:06:49 2017-03-13 21:2… Minneap…     188661\n3   6309 2008-09-13 22:22:33 2017-03-13 21:5… France       664389\n4   7552 2008-09-15 13:57:22 2017-03-13 01:1… Ottawa,…     129258\n5   8745 2008-09-15 16:47:12 2017-02-25 07:5… Calgary…      11418\n6  12711 2008-09-16 15:22:32 2017-03-13 21:5… Seattle…     248780\n# ℹ 5 more variables: displayname &lt;chr&gt;, upvotes &lt;int&gt;,\n#   downvotes &lt;int&gt;, age &lt;int&gt;, accountid &lt;int&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\ndplyr uses lazy evaluation when interfacing with databases – it only does the query and return results when the results are needed (in this case when we call head).",
    "crumbs": [
      "Big Data in R and Python"
    ]
  },
  {
    "objectID": "R-and-Python.html#manipulating-datasets-quickly-in-memory",
    "href": "R-and-Python.html#manipulating-datasets-quickly-in-memory",
    "title": "Working with big data in R and Python",
    "section": "3 Manipulating datasets quickly in memory",
    "text": "3 Manipulating datasets quickly in memory\n\n3.1 data.table in R\nThe data.table package provides a lot of functionality for fast manipulation of datasets in memory. data.table can do the standard SQL operations such as indexing, merges/joins, assignment, grouping, etc. Plus data.table objects are data frames (i.e., they inherit from data frames) so they are compatible with R code that uses data frames.\nIf you’ve got enough memory, data.table can be effective with pretty large datasets (e.g., 10s of gigabytes).\nTo illustrate without the example taking too long, we’ll only read in a subset of the Wikipedia webtraffic data.\nLet’s read in the dataset, specifying the column classes so that fread() doesn’t have to detect what they are (which will take additional time and might cause errors). Note that we can read directly from a UNIX operation piped into R.\n\nlibrary(data.table)\ncolClasses &lt;- c('numeric', 'numeric', 'character', \n           'character', 'numeric', 'numeric')\ncolNames &lt;- c('date', 'hour', 'site', 'page', 'count', 'size')\nsystem.time(wikiDT &lt;- fread('gzip -cd data/part-0000?.gz', \n col.names = colNames, colClasses = colClasses, header = FALSE,\n quote = \"\"))\n## 30 sec. for 300 MB zipped\n\nNow let’s do some basic subsetting. We’ll see that setting a key (equivalent to setting an index in SQL) can improve lookup speed dramatically.\n\n## without a key (i.e., index)\nsystem.time(sub &lt;- subset(wikiDT, count == 512)) # .27 sec.\nsystem.time(setkey(wikiDT, count , size)) # 3 sec.\n\n## with a key (i.e., index)\nsystem.time(sub2 &lt;- wikiDT[.(512), ]) # essentially instantaneous\n\ndata.table has a lot of functionality and can be used to do a variety of sophisticated queries and manipulations (including aggregation operations), but it has its own somewhat involved syntax and concepts. The above just scratches the surface of what you can do with it.\n\n\n3.2 Using dplyr syntax with data.table in R\nRather than learning the data.table syntax, one can also use dplyr syntax with data.table objects.\nWe can use dplyr syntax directly with data table objects, illustrated here with our existing wikiDT data table.\n\nsystem.time(sub &lt;- wikiDT %&gt;% filter(count == 512)) \n\nOne can also use dtplyr to set use a data table as a back end for dplyr manipulations. Using lazy_dt allows dtplyr to do some optimization as it generates the translation from dplyr syntax to data table syntax, though this simple example doesn’t illustrate the usefulness of that.\n\nwikiDT2 &lt;- lazy_dt(wikiDT)\nsystem.time(sub &lt;- wikiDT2 %&gt;% filter(count == 512)) # 0.1 sec.\n\nFinally the tidytable package also allows you to use dplyr syntax as well as other tidyverse syntax, such as tidyr functions.\n\n\n3.3 Polars dataframes in Python\nAs mentioned earlier, Polars is a newer dataframe package that provides a Python interface, operates in memory, and is designed to be fast. It uses the Arrow columnar format. It also provides a lazy execution model like Spark or Dask that allows for automatic optimization of queries.\n\n\n3.4 DuckDB\nWith DuckDB, you can run queries against existing R and Python data frames, collections of files in the Parquet file format and other file formats, and Arrow objects, without having to copy the data or import it into an actual database.\nIn R, use the duckdb_register and duckdb_register_arrow functions to ‘register’ the data frame or Arrow data source.\nFor Python, see the example syntax in the DuckDB documentation to query Pandas and Polars data frames and Arrow objects.\nAlternatively, you can read the data from files on disk into a DuckDB database table and then run queries against the database you’ve created.\n\nimport duckdb\ndata = duckdb.read_parquet(\"data/*.parquet\")\ndata.to_table(\"wikistats\")\nduckdb.sql(\"select * from wikistats limit 5\")\nduckdb.sql(\"select count(*) from wikistats\")",
    "crumbs": [
      "Big Data in R and Python"
    ]
  },
  {
    "objectID": "R-and-Python.html#working-with-large-datasets-on-disk",
    "href": "R-and-Python.html#working-with-large-datasets-on-disk",
    "title": "Working with big data in R and Python",
    "section": "4 Working with large datasets on disk",
    "text": "4 Working with large datasets on disk\nThere are a variety of packages in R and Python that allow you to work with very large datasets on disk without loading them fully into memory. Some of these are also very good at compressing files to reduce disk storage.\nI recommend first considering Arrow as it works well with the usual dataframe manipulations, but the other packages mentioned here may also be useful.\n\n4.1 Arrow\nApache Arrow provides efficient data structures for working with data in memory, usable in R via the arrow package and the PyArrow package in Python. Data are stored by column, with values in a column stored sequentially and in such a way that one can access a specific value without reading the other values in the column (O(1) lookup).\nIn general Arrow will only read data from disk as needed, avoiding keeping the entire dataset in memory (how much has to be read depends on the file format, with the native arrow format best in this regard), which can reduce I/O and memory usage.\nYou can use Apache Arrow to read and write from datasets stored as one or (often) more files in various formats, including:\n\nparquet: a space-efficient, standard format;\narrow format: data are stored in the same format on disk (called the ‘feather’ format) as in memory, improving I/O speed; and\ntext/csv files.\n\nSee this very useful discussion of file formats, comparing the parquet and arrow formats. And note that if you’re going to be reading the data frequently off disk, storing the files in text/CSV is not a good idea as it will be much faster to read from the Parquet or Arrow formats.\nHere’s a bit of what you can do with the PyArrow package for Python, illustrating working with data from a collection of Parquet files.\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\ntbl = pq.read_table(\"data/parquet\")\n## alternatively\n\ntbl = tbl.rename_columns([\"date\",\"hour\",\"lang\",\"site\",\"hits\",\"size\"])\ndf = tbl.filter(ds.field('hits') &gt; 10000).to_pandas()\n\nFor R, there’s a nice vignette covering basic usage, which involves dplyr syntax. Here’s an example of reading from a single file (at the moment, I’m not seeing how to read from multiple files):\n\ntbl &lt;- read_parquet(\"data/parquet/part-00000.parquet\")\ntbl &lt;- read_feather(\"data/arrow/part-00000.arrow\")\n\n\n\n4.2 DuckDB\nWith DuckDB, with some file formats (e.g., CSV, Parquet), you can run queries on files on disk without reading the entire dataset into memory.\nHere’s an example in Python.\n\nimport duckdb\nduckdb.sql(\"select * from 'data/*.parquet' limit 5\")\nduckdb.sql(\"select count(*) from 'data/*.parquet'\")\ndf = duckdb.sql(\"select * from 'data/*.parquet' limit 5\").to_df()\n\n\n\n4.3 fst\nThe fst package for R provides the ability to quickly read and write data frames in parallel from data stored on disk in the efficient fst format. A key feature in terms of reducing memory use is that data can be quickly accessed by column or by row (O(1) lookup), allowing one to easily subset the data when reading, rather than reading the entire dataset into memory, which is what would otherwise happen.\nHere’s an example, starting by reading data (some of the Wikipedia traffic data, 2.3 GB of data) into an initial data frame in R (which might defeat the purpose if the dataset size is too big to fit in memory).\n\n## read in Wikistats data\nwikiDF &lt;- readr::read_table(file = pipe(\"gzip -cd data/part-0000?.gz\"),\n        col_names = c('day','hour','language','site','hits','size'),\n        col_types = c('nnccnn'))\nsystem.time(write_fst(wikiDF, file.path('/tmp', 'data.fst')))  ## 8.9 seconds\n\nThe size of the compressed file is 790 MB based on the default compression, but one can choose different compression levels.\n\nsystem.time(wikiDF &lt;- read_fst(file.path('/tmp','data.fst')))  ## 8.4 seconds\n\nThe 8 seconds to read the data compares to 55 seconds to read the data from the gzipped files via a connection using readr::read_table and 29 seconds via data.table::fread.\n\n\n4.4 Additional packages in R (ff, LaF, bigmemory)\n\n4.4.1 ff\nff stores datasets in columnar format, with one file per column, on disk, so is not limited by memory (with the caveat below). It then provides fast access to the dataset from R.\nTo create the disk-based ff dataset, you’ll need to first read in the data from its original home. Note the arguments are similar to those for read.table and read.csv. read.table.ffdf reads the data in chunks.\n\nlibrary(ff)\ncolClasses &lt;- c('numeric','numeric','character', 'character','numeric','numeric')\ncolClasses[colClasses == 'character'] &lt;- 'factor'  # 'character' not allowed in ff\n## read in Wikistats data; this will take a while.\nwikiff &lt;- read.table.ffdf(file = pipe(\"gzip -cd data/part-0000?.gz\"),\n        colClasses = colClasses, sep = ' ')\n\nNow, one can save the ff dataset into permanent storage on disk that can be much more quickly loaded than the original reading of the data above.\n\nsystem.time(ffsave(wikiff, file = 'wikistats'))   ## 80 sec.\nrm(wikiff)\n\nHere’s how one loads the dataset back in.\n\nsystem.time(ffload('wikistats'))  ## 20 sec.\n\nIn the above operations, we wrote a copy of the file in the ff binary format that can be read more quickly back into R than the original reading of the CSV using ffsave and ffload. Also note the reduced size of the binary format file compared to the original CSV. It’s good to be aware of where the binary ff file is stored given that for large datasets, it will be large. With ff (I think bigmemory is different in how it handles this) it appears to be stored in /tmp in an R temporary directory. Note that as we work with large files we need to be more aware of the filesystem, making sure in this case that /tmp has enough space.\nTo use ff effectively, you want to use functions designed to manipulate ff objects; otherwise R will convert the ff dataset into a standard data frame and defeat the purpose as this will put the entire dataset in memory. You can look at the ff and ffbase packages to see what functions are available using library(help = ff) and library(help = ffbase). Notice that there is an merge.ff function for joins. Here we use the ff-specific table function:\n\ntable.ff(wikiff$hour)\n\n\nMiscellanea\nNote that a copy of an ff object appears to be a shallow copy: if you modify the copy it will change the data in the original object.\nNote that ff stores factor levels in memory, so if one has many factor levels, that can be a limitation. Furthermore, character columns are not allowed, so one is forced to use factors. Thus with textual data or the like, one can easily run into this limitation. With the Wikistats data, this is a big problem.\nAlso, I’ve encountered problems when there are more than about 1000 columns because each column is a separate file and there can be limitations in R on how many files it has open at once.\n\n\n\n4.4.2 LaF package\nThe LaF package is designed to quickly read in data from CSV and FWF (fixed-width format) input files, efficiently handling cases where you only want some of the rows or columns. It requires unzipped text files as input, so one can’t unzip input files on the fly via piping.\n\ncolClasses &lt;- c('numeric','numeric','character', 'character','numeric','numeric')\ncolNames &lt;- c('date', 'hour', 'site', 'page', 'count', 'size')\n## read in Wikistats data\ndatLaF &lt;- laf_open_csv(file.path('data', 'part-00000.txt'), sep = ' ',\n       column_types = colClasses, column_names = colNames)  ## returns immediately\nsub &lt;- datLaf[dat$count[] == 635,]\n\nIf you run this you’ll see that the laf_open_csv took no time, indicating LaF is using lazy evaluation.\n\n\n4.4.3 bigmemory for matrices\nbigmemory is similar to ff in providing the ability to load datasets into R without having them in memory, but rather stored in clever ways on disk that allow for fast access. bigmemory provides a big.matrix class, so it appears to be limited to datasets with a single type for all the variables. However, one nice feature is that one can use big.matrix objects with foreach (one of R’s parallelization tools) without passing a copy of the matrix to each worker. Rather the workers can access the matrix stored on disk.\nThe biglm package provides the ability to fit linear models and GLMs to big datasets, with integration with ff and bigmemory.\n\n\n\n4.5 Strategies in Python\nPython provides a variety of packages and approaches you can use to avoid reading large datasets fully into memory. Here is a brief overview of a few approaches:\n\nUse the Dask package to break up datasets into chunks. Dask processes the data in chunks, so one often doesn’t need a lot of memory, even just on one machine.\nUse numpy.load with the mmap_mode argument to access a numpy array (stored in a .npy file) on disk via memory mapping, reading only the pieces of the array that you need into memory, as discussed here.\n\nSee here for more discussion of accessing data on disk from Python.\n\n\n4.6 Online (batch) processing of data in R and Python\nAnother approach is to manually process the data in batches, only reading in chunks of data that can fit in memory before doing some computation or writing back out to disk and then reading in the next chunk. When taking this approach, you want to ensure that the code you are using will be able to skip directly to the point in the file where it should read the next chunk of data from (randomly accessing memory) rather than reading all the data up to the point of interest and simply discarding the initial data.\nNot surprisingly there is a ton more functionality than shown below (in both Python and R) for reading chunks from files as well as skipping ahead in a file via a file connection or stream.\n\n4.6.1 Online processing in R\nIn R, various input functions can read in a subset of a file or can skip ahead. In general the critical step is to use a connection rather than directly opening the file, as this will allow one to efficiently read the data in in chunks.\nI’ve put these in separate chunks as a reminder that for more accurate time comparisons they should be run in separate R sessions as there are some caching effects (though it’s surprising that closing R has an effect as I would think the file would be cached by the OS regardless).\nFirst we’ll see that skipping ahead when not using a connection is costly – R needs to read all the earlier rows before getting to the data of interest:\n\nfn &lt;- file.path('data', 'questions-2016.csv')\nsystem.time(dat1 &lt;- read.csv(fn, nrows = 100000, header = TRUE))  # 0.3 sec.\nsystem.time(dat2 &lt;- read.csv(fn, nrows = 100000, skip = 100001, header = FALSE)) # 0.5 sec.\nsystem.time(dat3 &lt;- read.csv(fn, nrows = 1, skip = 100001, header = FALSE)) # 0.15 sec.\nsystem.time(dat4 &lt;- read.csv(fn, nrows = 100000, skip = 1000001, header = FALSE)) # 3.7 sec.\n\nIf we use a connection, this cost is avoided (although there is still a cost to skipping ahead compared to reading in chunks, picking up where the last chunk left off):\n\nfn &lt;- file.path('data', 'questions-2016.csv')\ncon &lt;- file(fn, open = 'r')\nsystem.time(dat1c &lt;- read.csv(con, nrows = 100000, header = TRUE)) # 0.3 sec.\nsystem.time(dat2c &lt;- read.csv(con, nrows = 100000, header = FALSE)) # 0.3 sec.\nsystem.time(dat3c &lt;- read.csv(con, nrows = 1, header = FALSE)) # .001 sec.\nsystem.time(dat5c &lt;- read.csv(con, nrows = 1, skip = 100000, header = FALSE)) # .15 sec\n\nYou can use gzfile, bzfile, url, and pipe to open connections to zipped files, files on the internet, and inputs processed through UNIX-style piping.\nread_csv is generally somewhat faster and seems to be able to skip ahead efficiently even though it is not using a connection (which surprises me given that with a CSV file you don’t know how big each line is so one would think one needs to process through each line in some fashion).\n\nlibrary(readr)\nfn &lt;- file.path('data', 'questions-2016.csv')\nsystem.time(dat1r &lt;- read_csv(fn, n_max = 100000, col_names = TRUE))   # 0.4 sec.\nsystem.time(dat2r &lt;- read_csv(fn, n_max = 100000, skip = 100001, col_names = FALSE)) # 0.13 sec\nsystem.time(dat3r &lt;- read_csv(fn, n_max = 1, skip = 200001, col_names = FALSE)) # 0.07 sec\nsystem.time(dat4r &lt;- read_csv(fn, n_max = 100000, skip = 1000001, col_names = FALSE)) # 0.18 sec\n\nNote that read_csv can handle zipped inputs, but does not handle a standard text file connection.\n\n\n4.6.2 Online processing in Python\nPandas’ read_csv has similar functionality in terms of reading a fixed number of rows and skipping rows, and it can decompress zipped files on the fly.\n\nimport pandas as pd\nimport timeit\nfn = os.path.join('data', 'users-2016.csv')\n\n## here's the approach I'd recommend, as it's what 'chunksize' is intended for\nstart_time = timeit.default_timer()\nchunks = pd.read_csv(fn, chunksize = 100000, header = 0) # 0.003 sec.\nelapsed = timeit.default_timer() - start_time\nelapsed\ntype(chunks)\n\n## read first chunk\nstart_time = timeit.default_timer()\ndat1c = chunks.get_chunk()  \nelapsed = timeit.default_timer() - start_time\nelapsed  # 0.2 sec.\n\n## read next chunk\nstart_time = timeit.default_timer()\ndat2c = chunks.get_chunk()  # 0.25 sec.\nelapsed = timeit.default_timer() - start_time\nelapsed  # 0.2 sec.\n\n## this also works but is less elegant\nstart_time = timeit.default_timer()\ndat1 = pd.read_csv(fn, header = 0, nrows = 100000)  \nelapsed = timeit.default_timer() - start_time\nelapsed  # 0.3 sec.\n\nstart_time = timeit.default_timer()\ndat2 = pd.read_csv(fn, nrows = 100000, header = None, skiprows=100001)  \nelapsed = timeit.default_timer() - start_time\nelapsed  # 0.3 sec.",
    "crumbs": [
      "Big Data in R and Python"
    ]
  },
  {
    "objectID": "db-management.html",
    "href": "db-management.html",
    "title": "Database management",
    "section": "",
    "text": "We’ll illustrate some basic database management using a different example dataset that contains some data on webtraffic to Wikipedia pages. Note that the input file used here involved some pre-processing relative to the data you get the directly from the Wikistats dataset available through Amazon Web Services (AWS) because in the data posted on AWS, the datetime information is part of the filename, rather than field(s) in the table.\nYou can get the raw input files of Wikistats data here",
    "crumbs": [
      "Database Management"
    ]
  },
  {
    "objectID": "db-management.html#sqlite",
    "href": "db-management.html#sqlite",
    "title": "Database management",
    "section": "1 SQLite",
    "text": "1 SQLite\n\n1.1 Setting up a database and using the SQLite command line\nWith SQLite you don’t need to deal with all the permissions and administrative overhead of a client-server style of DBMS because an SQLite database is simply a file that you can access without a password or connecting to a database server process.\nTo start the SQLite interpreter in Linux, either operating on or creating a database named wikistats.db:\nsqlite3 wikistats.db\nHere’s the syntax to create an (empty) table:\ncreate table webtraffic (date char(8), hour char(6), site varchar, page varchar, count integer, size double precision);\n.quit\n\n\n1.2 Populating a table\nHere’s an example of reading from multiple files into SQLite using the command line. We create a file import.sql that has the configuration for the import:\n.separator \" \"\n.import /dev/stdin webtraffic\nThen we can iterate through our files from the UNIX shell, piping the output of gzip to the sqlite3 interpreter:\nfor file in $(ls part*gz); do\n    echo \"copying $file\"\n    gzip -cd $file | sqlite3 wikistats.db '.read import.sql'\ndone\nLet’s check that the records are in the database. We could do this from R or Python, but here we’ll do it in the SQLite interface:\nselect * from webtraffic limit 5;\n\n\n1.3 Data cleaning\nA problem in this example with importing from the data files into SQLite as above is the presence of double quote (“) characters that are not meant to delineate strings but are actually part of a field. In this case probably the easiest thing is simply to strip out those quotes from UNIX. Here we use sed to search and replace to create versions of the input files that don’t have the quotes.\n\nfor file in $(ls *gz); do\n    gzip -cd ${file} | sed  \"s/\\\"//g\" | gzip -c &gt; wikistats-cleaned/${file}\ndone\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you want to read the data into SQLite yourself, you will need to do something about the quotes; I haven’t stripped them out of the files.",
    "crumbs": [
      "Database Management"
    ]
  },
  {
    "objectID": "db-management.html#duckdb",
    "href": "db-management.html#duckdb",
    "title": "Database management",
    "section": "2 DuckDB",
    "text": "2 DuckDB\nDuckDB also has an interpreter (a command line interface) that you can run from the command line instead of using it via R or Python or other languages.\nI won’t demonstrate that here. Instead I’ll demonstrate creation of a database directly from R without having to read the data into memory in R. We control the schema from R as well using standard arguments of read.csv() (which is used behind the scenes in setting up the DuckDB database but not for reading in all the data).\n\nlibrary(duckdb)\ndbname &lt;- \"wikistats.duckdb\"\ndrv &lt;- duckdb()\ndb &lt;- dbConnect(drv, dbname)\nduckdb_read_csv(db, 'webtraffic', list.files('.', pattern = \"^part-\"), \n                    delim = ' ', header = FALSE, na.strings = 'NA', \n                    colClasses = c('character','character','character','character','integer','numeric'), \n                    col.names = c('date','hour','site','page','count','size')))\ndbDisconnect(db, shutdown = TRUE)\n\nThe DuckDB Python interface has read_csv and read_parquet functions for creating a database from one or more data files.",
    "crumbs": [
      "Database Management"
    ]
  },
  {
    "objectID": "db-management.html#postgresql",
    "href": "db-management.html#postgresql",
    "title": "Database management",
    "section": "3 PostgreSQL",
    "text": "3 PostgreSQL\n\n3.1 Setting up a database and using the Postgres command line\nFirst make sure Postgres is installed on your machine.\nOn Ubuntu, you can install Postgres easily via apt-get:\nsudo apt-get install postgresql postgresql-contrib\nNext we’ll see how to set up a database. You’ll generally need to operate as the postgres user for these sorts of manipulations. Of course if you’re just a user accessing an existing database and existing tables, you don’t need to worry about this.\nsudo -u postgres -i  # become the postgres user\npsql  # start postgres interpreter\nNow from within the Postgres interpreter, you can create a database, tables within the database, and authenticate users to do things with those tables.\ncreate database wikistats;\ncreate user paciorek with password 'test';\ngrant all privileges on database wikistats to paciorek;\nPostgreSQL and other DBMS (not SQLite) allow various kinds of control over permissions to access and modify databases and tables as well. It can get a bit involved because the administrator has fine-grained control over what each user can do/access.\nNow let’s create a table in the database, after first connecting to the specific database so as to operate on it.\n\\connect wikistats\ncreate table webtraffic (date char(8), hour char(6), site varchar, page varchar,\n       count integer, size double precision);\ngrant all privileges on table webtraffic to paciorek;\n\\quit\n\n\n\n\n\n\nNote\n\n\n\nNotice the use of \\ to do administrative tasks (as opposed to executing SQL syntax), and the use of ; to end each statement. Without the semicolon, Postgres will return without doing anything.\n\n\nIf you want control over where the database is stored (you probably only need to worry about this if you are creating a large database), you can do things like this:\nshow data_directory;\ncreate tablespace dbspace location '/var/tmp/pg';\ncreate database wikistats tablespace dbspace;\ncreate user paciorek with password 'test';\ngrant all privileges on database wikistats to paciorek;\n\n\n3.2 Populating a table\nHere’s an example of importing a single file into Postgres from within the psql interpreter running as the special postgres user. In this case we have space-delimited text files. You can obtain the file part-00000 as discussed in the introduction (you’ll need to run gunzip part-00000.gz first).\n\\connect wikistats\ncopy webtraffic from 'part-00000' delimiter ' ';\nIf one had CSV files, one could do the following\ncopy webtraffic from 'part-00000' csv;\nTo actually handle the Wikistats input files, we need to deal with backslash characters occurring at the end of text for a given column in some rows. Ordinarily in standard Postgres ‘text’ format (different from Postgres ‘csv’ format), a backslash is used to ‘quote’ characters that would usually be treated as row or column delimiters (i.e., preceding such a character by a backslash means it is treated as a character that is part of the field). But we just want the backslash treated as a character itself. So we need to tell Postgres not to treat a backslash as the quoting character. To do that we specify the quote character. However, the quote keyword is only provided when importing ‘csv’ format. In ‘csv’ format the double-quote character is by default treated as delineating the beginning and end of text in a field, but the Wikistats files have double-quotes as part of the fields. So we need to set the quote character as neither a double-quote nor a backslash. The following syntax does that by specifying that the quote character is a character (\\b) that never actually appears in the file. The ‘e’ part is so that Postgres treats \\b as a single character, i.e., ‘escaping’ the backslash, and the ‘csv’ is because the quote keyword only works with the csv format, but note that by setting the delimiter to a space, it’s not really a CSV file!\ncopy webtraffic from 'part-00000' delimiter ' ' quote e'\\b' csv;\nOften you’ll need to load data from a large number of possibly zipped text files. As an example of how you would load data in a case like that, here’s some shell scripting that will iterate through multiple (gzipped) input files of Wikistats data, running as the regular user:\nexport PGPASSWORD=test  # set password via UNIX environment variable\nfor file in $(ls part*gz); do  # loop thru files whose names start with 'part' and end with 'gz'\n  echo \"copying $file\"\n  ## unzip and then pass by UNIX pipe to psql run in non-interactive mode\n  gzip -cd $file |\n    psql -d wikistats -h localhost -U paciorek -p 5432 -c \"\\copy webtraffic from stdin delimiter ' ' quote e'\\b' csv\"\ndone\n\n\n\n\n\n\nNote\n\n\n\nUsing \\copy as above invokes the psql copy command (copy would invoke the standard SQL copy command), which allows one to operate as a regular user and to use relative paths. In turn \\copy invokes copy in a specific way.\n\n\n\n\n3.3 Data cleaning\nOne complication is that often the input files will have anomalies in them. Examples include missing columns for some rows, individual elements in a column that are not of the correct type (e.g., a string in a numeric column), and characters that can’t be handled. In the Wikistats data case, one issue was lines without the full set of columns and another was the presence of a backslash character at the end of the text for a column.\nWith large amounts of data or many files, this can be a hassle to deal with. UNIX shell commands can sometimes be quite helpful, including use of sed and awk. Or one might preprocess files in chunks using Python.\nFor example the following shell scripting loop over Wikistats files ensures each row has 6 fields/columns by pulling out only rows with the full set of columns. I used this to process the input files before copying into Postgres as done above. Actually there was even more preprocessing because in the form of the data available from Amazon’s storage service, the date/time information was part of the filename and not part of the data files.\n\nfor file in $(ls *gz); do\n    gzip -cd $file | grep \"^.* .* .* .* .* .*$\" | gzip -c &gt; ../wikistats-fulllines/$file\ndone\n\nNote that this restriction to rows with a full set of fields has already been done in the data files I provide to you.",
    "crumbs": [
      "Database Management"
    ]
  },
  {
    "objectID": "db-management.html#database-administration-and-configuration-miscellanea",
    "href": "db-management.html#database-administration-and-configuration-miscellanea",
    "title": "Database management",
    "section": "3 Database administration and configuration miscellanea",
    "text": "3 Database administration and configuration miscellanea\nYou can often get configuration information by making a query. For example, here’s how one can get information on the cache size in SQLite or on various settings in Postgres.\n\n# SQLite\ndbGetQuery(db, \"pragma cache_size\")\ndbGetQuery(db, \"pragma cache_size=90000\")\n# sets cache size to ~90 GB, 1 KB/page, but not really relevant as\n# operating system should do disk caching automatically\n\n# Postgres\ndbGetQuery(db, \"select * from pg_settings\")\ndbGetQuery(db, \"select * from pg_settings where name='dynamic_shared_memory_type'\")",
    "crumbs": [
      "Database Management"
    ]
  },
  {
    "objectID": "db-management.html#remote-access-to-postgresql-databases",
    "href": "db-management.html#remote-access-to-postgresql-databases",
    "title": "Database management",
    "section": "4 Remote access to PostgreSQL databases",
    "text": "4 Remote access to PostgreSQL databases\nIf you want to connect to a Postgres database running on a different machine, here’s one approach that involves SSH port forwarding. For example, you could connect to a Postgres database running on some server while working as usual in R or Python on your laptop.\nFirst, on your machine, set up the port forwarding where 63333 should be an unused port on your local machine and PostgresHostMachine is the machine on which the database is running.\nFor Linux/Mac, from the terminal:\nssh -L 63333:localhost:5432 yourUserName@PostgresHostMachine\nUsing Putty on Windows, go to ‘Connection -&gt; SSH -&gt; Tunnels’ and put ‘63333’ as the ‘Source port’ and ‘127.0.0.1:5432’ as the ‘Destination’. Click ‘Add’ and then connect to the machine via Putty.\nIn either case, the result is that port 63333 on your local machine is being forwarded to port 5432 (the standard port used by Postgres) on the server. The use of ‘localhost’ is a bit confusing - it means that you are forwarding port 63333 to port 5432 on ‘localhost’ on the server.\nThen (on your local machine) you can connect by specifying the port on your local machine, with the example here being from R:\n\ndb &lt;- dbConnect(drv, dbname = 'wikistats', user = 'yourUserName', \n   password = 'yourPassword', host = 'localhost', port = 63333)",
    "crumbs": [
      "Database Management"
    ]
  },
  {
    "objectID": "db-management.html#unix-tools-for-examining-disk-access-io-and-memory-use",
    "href": "db-management.html#unix-tools-for-examining-disk-access-io-and-memory-use",
    "title": "Database management",
    "section": "5 UNIX tools for examining disk access (I/O) and memory use",
    "text": "5 UNIX tools for examining disk access (I/O) and memory use\n\n5.1 I/O\niotop shows disk input/output in real time on a per-process basis, while iostat shows overall disk use.\n\niotop    # shows usage in real time\niostat 1 # shows usage every second\n\n\n\n5.2 Memory\nTo see how much memory is available, one needs to have a clear understanding of disk caching. As discussed above, the operating system will generally cache files/data in memory when it reads from disk. Then if that information is still in memory the next time it is needed, it will be much faster to access it the second time around. While the cached information is using memory, that same physical memory is immediately available to other processes, so the memory is available even though it is in use.\nWe can see this via free -h (the -h is for ‘human-readable’, i.e. show in GB (G)).\n              total        used        free      shared  buff/cache   available\nMem:           251G        998M        221G        2.6G         29G        247G\nSwap:          7.6G        210M        7.4G\nYou’ll generally be interested in the Memory row. (See below for some comments on Swap.) The shared column is complicated and probably won’t be of use to you. The buff/cache column shows how much space is used for disk caching and related purposes but is actually available. Hence the available column is the sum of the free and buff/cache columns (more or less). In this case only about 1 GB is in use (indicated in the used column).\ntop and vmstat both show overall memory use, but remember that the amount available is the amount free plus any buffer/cache usage. Here is some example output from vmstat:\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 1  0 215140 231655120 677944 30660296    0    0     1     2    0    0 18  0 82  0  0\nIt shows 232 GB free and 31 GB used for cache and therefore available, for a total of 263 GB available.\nHere are some example lines from top:\nKiB Mem : 26413715+total, 23180236+free,   999704 used, 31335072 buff/cache\nKiB Swap:  7999484 total,  7784336 free,   215148 used. 25953483+avail Mem \nWe see that this machine has 264 GB RAM (the total column in the Mem row), with 259.5 GB available (232 GB free plus 31 GB buff/cache as seen in the Mem row). (I realize the numbers don’t quite add up for reasons I don’t fully understand, but we probably don’t need to worry about that degree of exactness.) Only 1 GB is in use.\nswap is essentially the reverse of disk caching. It is disk space that is used for memory when the machine runs out of physical memory. You never want your machine to be using swap for memory, because your jobs will slow to a crawl. Here the swap line in both free and top shows 8 GB swap space, with very little in use, as desired.",
    "crumbs": [
      "Database Management"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "",
    "text": "This tutorial covers tools for manipulating large datasets, including those living in SQL databases or in data frames and related objects in R and Python. The focus is on querying rather than creating and administering databases as the intended audience is for statisticians/data analysts/data scientists who are carrying out analyses. A major emphasis is on how to do queries efficiently and how to use SQL effectively. At the moment, this tutorial is somewhat more focused on R than Python, but the manipulation of databases from R and Python are very similar because the core reliance is on SQL.\nThis tutorial assumes you have a working knowledge of R or Python.\n\n\nMaterials for this tutorial, including the Markdown files and associated code files that were used to create these documents are available on GitHub.\nThe example data files are not part of the GitHub repository. You can get the example data files (both Stack Overflow data for 2021 and Wikipedia webtraffic data for the year 2008) here.\nSolutions to the SQL challenges are available on request.\n\n\n\n\n\nThe simplest way to use a database is with SQLite, a lightweight database engine under which the database is stored simply in a single file.\nBoth R and Python can easily interact with an SQLite database. For R you’ll need the RSQLite package. For Python you’ll need the sqlite3 package.\nOne thing to note is that SQLite does not have some useful functionality that other databas management systems have. For example, you can’t use ALTER TABLE to modify column types or drop columns.\n\n\n\nDuckDB is another lightweight database engine under which the database is stored simply in a single file. It stores data column-wise, which can lead to big speedups when doing queries operating on large portions of tables (so-called “online analytical processing” (OLAP)).\nBoth R and Python can easily interact with a DuckDB database. For R you’ll need the duckdb R package. For Python you’ll need the duckdb Python package.\n\n\n\nTo replicate the (non-essential) PostgreSQL administration portion of this tutorial, you’ll need access to a machine on which you can run a PostgreSQL server. While there are a variety of ways to do this, this tutorial assumes that you are running PostgreSQL on an Ubuntu (or Debian) Linux machine. If you are a Windows or Mac user, there are several options for accessing a Linux environment:\n\nYou could run Ubuntu in a Docker container; Docker can be installed on Windows or Mac. Once you’ve installed Docker and have access to a terminal command line, please see the commands in docker.sh in this repository.\nYou could run an Amazon EC2/Google Cloud/Azure virtual machine instance, using a image that supports R and/or Python and then installing PostgreSQL as discussed in this tutorial.\nThe big cloud computing providers have created a wide array of specific database services, so if you are using a cloud provider, you’d probably want to take advantage of those rather than ‘manually’ running a database via a virtual machine.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#this-tutorial",
    "href": "index.html#this-tutorial",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "",
    "text": "This tutorial covers tools for manipulating large datasets, including those living in SQL databases or in data frames and related objects in R and Python. The focus is on querying rather than creating and administering databases as the intended audience is for statisticians/data analysts/data scientists who are carrying out analyses. A major emphasis is on how to do queries efficiently and how to use SQL effectively. At the moment, this tutorial is somewhat more focused on R than Python, but the manipulation of databases from R and Python are very similar because the core reliance is on SQL.\nThis tutorial assumes you have a working knowledge of R or Python.\n\n\nMaterials for this tutorial, including the Markdown files and associated code files that were used to create these documents are available on GitHub.\nThe example data files are not part of the GitHub repository. You can get the example data files (both Stack Overflow data for 2021 and Wikipedia webtraffic data for the year 2008) here.\nSolutions to the SQL challenges are available on request.\n\n\n\n\n\nThe simplest way to use a database is with SQLite, a lightweight database engine under which the database is stored simply in a single file.\nBoth R and Python can easily interact with an SQLite database. For R you’ll need the RSQLite package. For Python you’ll need the sqlite3 package.\nOne thing to note is that SQLite does not have some useful functionality that other databas management systems have. For example, you can’t use ALTER TABLE to modify column types or drop columns.\n\n\n\nDuckDB is another lightweight database engine under which the database is stored simply in a single file. It stores data column-wise, which can lead to big speedups when doing queries operating on large portions of tables (so-called “online analytical processing” (OLAP)).\nBoth R and Python can easily interact with a DuckDB database. For R you’ll need the duckdb R package. For Python you’ll need the duckdb Python package.\n\n\n\nTo replicate the (non-essential) PostgreSQL administration portion of this tutorial, you’ll need access to a machine on which you can run a PostgreSQL server. While there are a variety of ways to do this, this tutorial assumes that you are running PostgreSQL on an Ubuntu (or Debian) Linux machine. If you are a Windows or Mac user, there are several options for accessing a Linux environment:\n\nYou could run Ubuntu in a Docker container; Docker can be installed on Windows or Mac. Once you’ve installed Docker and have access to a terminal command line, please see the commands in docker.sh in this repository.\nYou could run an Amazon EC2/Google Cloud/Azure virtual machine instance, using a image that supports R and/or Python and then installing PostgreSQL as discussed in this tutorial.\nThe big cloud computing providers have created a wide array of specific database services, so if you are using a cloud provider, you’d probably want to take advantage of those rather than ‘manually’ running a database via a virtual machine.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "2 Background",
    "text": "2 Background\n\n2.1 Data size\nThe techniques and tools discussed here are designed for datasets in the range of gigabytes to tens of gigabytes, though they may scale to larger if you have a machine with a lot of memory or simply have enough disk space and are willing to wait. If you have 10s of gigabytes of data, you’ll be better off if your machine has 10s of GBs of memory, as discussed in this tutorial.\nIf you’re scaling to 100s of GBs, terabytes or petabytes, using the cloud computing providers’ tools for working with big datasets is probably your best bet (e.g., Amazon RedShift or Google BigQuery), or possibly carefully-administered databases. This tutorial has some information on using Google BigQuery, and this tutorial will be useful if you’re doing SQL queries on professionally-administered databases or databases in the cloud or in a Spark context.\n\n\n2.2 Memory vs. disk\nOn a computer there is a hierarchy of locations where data can be stored. The hierarchy has the trade-off that the locations that the CPU can access most quickly can store the least amount of data. The hierarchy looks like this:\n\ncpu cache\nmain memory\ndisk\nlocal network (data stored on other machines)\ngeneral internet access\n\nFor our purposes here the key question is whether the data resides in memory or on disk, but when considering Spark and distributed systems, one gets into issues of moving data across the network between machines.\nFormally, databases are stored on disk, while R and Python store datasets in memory. This would suggest that databases will be slow to access their data but will be able to store more data than can be loaded into an R or Python session. However, databases can be quite fast due in part to disk caching by the operating system as well as careful implementation of good algorithms for database operations. For more information about disk caching see the database management document.\nAnd conversely, R and Python have mechanisms for storing large datasets on disk in a way that they can be accessed fairly quickly.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#database-systems-and-sql",
    "href": "index.html#database-systems-and-sql",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "3 Database systems and SQL",
    "text": "3 Database systems and SQL\n\n3.1 Overview of databases\nBasically, standard SQL databases are relational databases that are a collection of rectangular format datasets (tables, also called relations), with each table similar to R or Pandas data frames, in that a table is made up of columns, which are called fields or attributes, each containing a single type (numeric, character, date, currency, enumerated (i.e., categorical), …) and rows or records containing the observations for one entity. Some of these tables generally have fields in common so it makes sense to merge (i.e., join) information from multiple tables. E.g., you might have a database with a table of student information, a table of teacher information and a table of school information.\nOne principle of databases is that if a set of fields contain duplicated information about a given category, you can more efficiently store information about each level of the category in a separate table. Consider information about people living in a state and information about each state - you don’t want to include variables that only vary by state in the table containing information about individuals (at least until you’re doing the actual analysis that needs the information in a single table). Or consider students nested within classes nested within schools.\nDatabases are set up to allow for fast querying and merging (called joins in database terminology).\nYou can interact with databases in a variety of database systems (DBMS=database management system). Some popular systems are SQLite, MySQL, PostgreSQL, Oracle and Microsoft Access. We’ll concentrate on accessing data in a database rather than management of databases. SQL is the Structured Query Language and is a special-purpose high-level language for managing databases and making queries. Variations on SQL are used in many different DBMS.\nQueries are the way that the user gets information (often simply subsets of tables or information merged across tables). The result of an SQL query is in general another table, though in some cases it might have only one row and/or one column.\nMany DBMS have a client-server model. Clients connect to the server, with some authentication, and make requests (i.e., queries).\nThere are often multiple ways to interact with a DBMS, including directly using command line tools provided by the DBMS or via Python or R, among others.\n\n3.1.1 Relational Database Management Systems (DBMS)\nThere are a variety of relational database management systems (DBMS). Some that are commonly used by the intended audience of this tutorial are SQLite, PostgreSQL, and mySQL. We’ll concentrate on SQLite and DuckDB (because they are simple to use on a single machine) and PostgreSQL (because is is a popular open-source DBMS that is a good representative of a client-server model and has some functionality that SQLite lacks).\n\n\n3.1.2 Serverless DBMS\nSQLite and DuckDB are quite nice in terms of being self-contained - there is no server-client model, just a single file on your hard drive that stores the database. There is no database process running on the computer; rather SQLite or DuckDB are embedded within the host process (e.g., Python or R). (There are also command line interfaces (CLI) for both SQLite and DuckDB that you can start from the command line/terminal.)\n\n\n3.1.3 NoSQL databases\nNoSQL (not only SQL) systems have to do with working with datasets that are not handled well in traditional DBMS, and not specifically about the use or non-use of SQL itself. In particular data might not fit well within the rectangular row-column data model of one or more tables in a database. And one might be in a context where a full DBMS is not needed. Or one might have more data or need faster responses than can be handled well by standard DBMS.\nWhile these systems tend to scale better, they generally don’t have a declarative query language so you end up having to do more programming yourself. For example in the Stanford database course referenced at the end of this tutorial, the noSQL video gives the example of web log data that records visits to websites. One might have the data in the form of files and not want to go through the trouble of data cleaning and extracting fields from unstructured text. In addition, one may need to do only simple queries that involve looking at each record separately and therefore can be easily done in parallel, which noSQL systems tend to be designed to do. Or one might have document data, such as Wikipedia pages, where the unstructured text on each page is not really suited for a DBMS.\nSome NoSQL systems include\n\nHadoop/Spark-style MapReduce systems,\nkey-value storage systems (e.g., with data stored as pairs of keys (i.e., ids) and values, such as in JSON),\ndocument storage systems (like key-value systems but where the value is a document), and\ngraph storage systems (e.g., for social networks).\n\n\n\n\n3.2 Databases in the cloud\nThe various big cloud computing providers (AWS, Google Cloud Platform (GCP), Azure) provide a dizzying array of different database-like services. Here are some examples.\n\nOnline database hosting services allow you to host databases (e.g., PostgreSQL databases) the infrastructure of a cloud provider. You basically manage the database in the cloud instead of on a physical machine. One example is Google Cloud SQL.\nData warehouses such as Google BigQuery and Amazon RedShift allow you to create a data repository in which the data are structured like in a database (tables, fields, etc.), stored in the cloud, and queried efficiently (and in parallel) using the cloud provider’s infrastructure). Storage is by column, which allows for efficient queries when doing queries operating on large portions of tables.\nData lakes store data in a less structured way in cloud storage in files (e.g., CSV, Parquet, Arrow, etc.) that generally have common structure. The data can be queried without creating an actual database or data warehouse.\n\nGoogle’s BigQuery has the advantages of not requiring a lot of administration/configuration while allowing your queries to take advantage of a lot of computing power. BigQuery will determine how to run a query in parallel across multiple (virtual) cores. You can see a demonstration of BigQuery.\n\n\n3.3 SQL\nSQL is a declarative language that tells the database system what results you want. The system then parses the SQL syntax and determines how to implement the query.\nLater we’ll introduce a database of Stack Overflow questions and answers.\nHere is a simple query that selects the first five rows (and all columns, based on the * wildcard) from a table (called ‘questions’) in a database that one has connected to:\nselect * from questions limit 5",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#schema-and-normalization",
    "href": "index.html#schema-and-normalization",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "4 Schema and normalization",
    "text": "4 Schema and normalization\nTo truly leverage the conceptual and computational power of a database you’ll want to have your data in a normalized form, which means spreading your data across multiple tables in such a way that you don’t repeat information unnecessarily.\nThe schema is the metadata about the tables in the database and the fields (and their types) in those tables.\nLet’s consider this using an educational example. Suppose we have a school with multiple teachers teaching multiple classes and multiple students taking multiple classes. If we put this all in one table organized per student, the data might have the following fields:\n\nstudent ID\nstudent grade level\nstudent name\nclass 1\nclass 2\n…\nclass n\ngrade in class 1\ngrade in class 2\n…\ngrade in class n\nteacher ID 1\nteacher ID 2\n…\nteacher ID n\nteacher department 1\nteacher department 2\n…\nteacher department n\nteacher age 1\nteacher age 2\n…\nteacher age n\n\nThere are a lot of problems with this.\n\n‘n’ needs to be the maximum number of classes a student might take. If one ambitious student takes many classes, there will be a lot of empty data slots.\nAll the information about individual teachers (department, age, etc.) is repeated many times, meaning we use more storage than we need to.\nIf we want to look at the data on a per teacher basis, this is very poorly organized for that.\nIf one wants to change certain information (such as the age of a teacher) one needs to do it in many locations, which can result in errors and is inefficient.\n\nIt would get even worse if there was a field related to teachers for which a given teacher could have multiple values (e.g., teachers could be in multiple departments). This would lead to even more redundancy - each student-class-teacher combination would be crossed with all of the departments for the teacher (so-called multivalued dependency in database theory).\nAn alternative organization of the data would be to have each row represent the enrollment of a student in a class, with as many rows per student as the number of classes the student is taking.\n\nstudent ID\nstudent name\nclass\ngrade in class\nstudent grade level\nteacher ID\nteacher department\nteacher age\n\nThis has some advantages relative to our original organization in terms of not having empty data slots, but it doesn’t solve the other three issues above.\nInstead, a natural way to order this database is with the following tables.\n\nStudent\n\nID\nname\ngrade_level\n\nTeacher\n\nID\nname\ndepartment\nage\n\nClass\n\nID\ntopic\nclass_size\nteacher_ID\n\nClassAssignment\n\nstudent_ID\nclass_ID\ngrade\n\n\nThen we do queries to pull information from multiple tables. We do the joins based on ‘keys’, which are the fields in each table that allow us to match rows from different tables.\n(That said, if all anticipated uses of a database will end up recombining the same set of tables, we may want to have a denormalized schema in which those tables are actually combined in the database. It is possible to be too pure about normalization! We can also create a virtual table, called a view, as discussed later.)\n\n4.1 Keys\nA key is a field or collection of fields that give(s) a unique value for every row/observation. A table in a database should then have a primary key that is the main unique identifier used by the DBMS. Foreign keys are columns in one table that give the value of the primary key in another table. When information from multiple tables is joined together, the matching of a row from one table to a row in another table is generally done by equating the primary key in one table with a foreign key in a different table.\nIn our educational example, the primary keys would presumably be: Student.ID, Teacher.ID, Class.ID, and for ClassAssignment two fields: {ClassAssignment.studentID, ClassAssignment.class_ID}.\nSome examples of foreign keys would be:\n\nstudent_ID as the foreign key in ClassAssignment for joining with Student on Student.ID\nteacher_ID as the foreign key in Class for joining with Teacher based on Teacher.ID\nclass_ID as the foreign key in ClassAssignment for joining with Class based on Class.ID",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#stack-overflow-example-database",
    "href": "index.html#stack-overflow-example-database",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "5 Stack Overflow example database",
    "text": "5 Stack Overflow example database\nI’ve obtained data from Stack Overflow, the popular website for asking coding questions, and placed it into a normalized database. We’ll explore SQL functionality using this example database, which has metadata (i.e., it lacks the actual text of the questions and answers) on all of the questions and answers posted in 2021.\nLet’s consider the Stack Overflow data. Each question may have multiple answers and each question may have multiple (topic) tags.\nIf we tried to put this into a single table, the fields could look like this if we have one row per question:\n\nquestion ID\nID of user submitting question\nquestion title\ntag 1\ntag 2\n…\ntag n\nanswer 1 ID\nID of user submitting answer 1\nanswer 2 ID\nID of user submitting answer 2\n…\n\nor like this if we have one row per question-answer pair:\n\nquestion ID\nID of user submitting question\nquestion title\ntag 1\ntag 2\n…\ntag n\nanswer ID\nID of user submitting answer\n\nAs we’ve discussed neither of those schema is particularly desirable.\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you devise a schema to normalize the data. I.e., what set of tables do you think we should create?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nDon’t peek until after you’ve thought about it, but you can view one reasonable schema here. The lines between tables indicate the relationship of foreign keys in one table to primary keys in another table. The schema in the actual databases of Stack Overflow data we’ll use in this tutorial is similar to but not identical to that.\n\n\n\n\n5.1 Getting the database\nYou can download a copy of the SQLite version of the Stack Overflow database (only data for the year 2021) from here as part of the overall zip with all of the example datasets as discussed in the introduction of this tutorial.\nIn the next section I’ll assume the .db file is placed in the subdirectory of the repository called data.\nNote that all of the code used to download the data from the Stack Overflow website and to manipulate it to create a complete Postgres database and (for the year 2021 only) an SQLite database and CSVs for each table is in the data/prep_stackoverflow subdirectory of this repository. Note that as of July 2025, the data were still being kept up to date online.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#accessing-a-database-and-using-sql-from-other-languages",
    "href": "index.html#accessing-a-database-and-using-sql-from-other-languages",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "6 Accessing a database and using SQL from other languages",
    "text": "6 Accessing a database and using SQL from other languages\nAlthough DBMS have their own interfaces (we’ll see a bit of this later), databases are commonly accessed from other programs. For data analysts this would often be Python or R, as seen next.\nMost of our examples of making SQL queries on a database will be done from R, but they could just as easily have been done from Python or other programs.\n\n6.1 Using SQL from R\nThe DBI package provides a front-end for manipulating databases from a variety of DBMS (SQLite, DuckDB, MySQL, PostgreSQL, among others). Basically, you tell the package what DBMS is being used on the back-end, link to the actual database, and then you can use the standard functions in the package regardless of the back-end.\nWith SQLite and DuckDB, R processes make calls against the stand-alone SQLite database (.db or .duckdb) file, so there are no SQLite-specific processes. With PostgreSQL, R processes call out to separate Postgres processes; these are started from the overall Postgres background process.\nApart from the different manner of connecting, all of the queries below are the same regardless of whether the back-end DBMS is SQLite, DuckDB, PostgreSQL, etc.\n\n6.1.1 SQLite\nYou can access and navigate an SQLite database from R as follows.\n\nlibrary(RSQLite)\ndrv &lt;- dbDriver(\"SQLite\")\ndir &lt;- 'data' # relative or absolute path to where the .db file is\ndbFilename &lt;- 'stackoverflow-2021.db'\ndb &lt;- dbConnect(drv, dbname = file.path(dir, dbFilename))\n# simple query to get 5 rows from a table\ndbGetQuery(db, \"select * from questions limit 5\")  \n\n  questionid        creationdate score viewcount answercount\n1   65534165 2021-01-01 22:15:54     0       112           2\n2   65535296 2021-01-02 01:33:13     2      1109           0\n3   65535910 2021-01-02 04:01:34    -1       110           1\n4   65535916 2021-01-02 04:03:20     1        35           1\n5   65536749 2021-01-02 07:03:04     0       108           1\n  commentcount favoritecount                               title\n1            0            NA     Can't update a value in sqlite3\n2            0            NA Install and run ROS on Google Colab\n3            8             0       Operators on date/time fields\n4            0            NA          Plotting values normalised\n5            5            NA     Export C# to word with template\n   ownerid\n1 13189393\n2 14924336\n3   651174\n4 14695007\n5 14899717\n\n\nWe can easily see the tables and their fields:\n\ndbListTables(db)\n\n[1] \"answers\"        \"questions\"      \"questions_tags\"\n[4] \"users\"         \n\ndbListFields(db, \"questions\")\n\n[1] \"questionid\"    \"creationdate\"  \"score\"        \n[4] \"viewcount\"     \"answercount\"   \"commentcount\" \n[7] \"favoritecount\" \"title\"         \"ownerid\"      \n\ndbListFields(db, \"answers\")\n\n[1] \"answerid\"     \"questionid\"   \"creationdate\" \"score\"       \n[5] \"ownerid\"     \n\n\nOne can either make the query and get the results in one go or make the query and separately fetch the results. Here we’ve selected the first five rows (and all columns, based on the * wildcard) and brought them into R as a data frame.\n\nresults &lt;- dbGetQuery(db, 'select * from questions limit 5')\nclass(results)\n\n[1] \"data.frame\"\n\nquery &lt;- dbSendQuery(db, \"select * from questions\")\nquery\n\n&lt;SQLiteResult&gt;\n  SQL  select * from questions\n  ROWS Fetched: 0 [incomplete]\n       Changed: 0\n\nresults2 &lt;- fetch(query, 5)\nidentical(results, results2)\n\n[1] TRUE\n\ndbClearResult(query)  # clear to prepare for another query\n\nTo disconnect from the database:\n\ndbDisconnect(db)\n\n\n\n6.1.2 DuckDB\nYou can access and navigate an DuckDB database from R as follows.\n\nlibrary(duckdb)\ndrv &lt;- duckdb()\ndir &lt;- 'data' # relative or absolute path to where the .db file is\ndbFilename &lt;- 'stackoverflow-2021.duckdb'\ndbd &lt;- dbConnect(drv, file.path(dir, dbFilename))\n# simple query to get 5 rows from a table\ndbGetQuery(dbd, \"select * from questions limit 5\")  \n\n  questionid            creationdate score viewcount answercount\n1   65534165 2021-01-01T22:15:54.657     0       112           2\n2   65535296 2021-01-02T01:33:13.543     2      1109           0\n3   65535910 2021-01-02T04:01:34.137    -1       110           1\n4   65535916 2021-01-02T04:03:20.027     1        35           1\n5   65536749 2021-01-02T07:03:04.783     0       108           1\n  commentcount favoritecount                               title\n1            0            NA     Can't update a value in sqlite3\n2            0            NA Install and run ROS on Google Colab\n3            8             0       Operators on date/time fields\n4            0            NA          Plotting values normalised\n5            5            NA     Export C# to word with template\n   ownerid\n1 13189393\n2 14924336\n3   651174\n4 14695007\n5 14899717\n\n\nTo disconnect from the database:\n\ndbDisconnect(dbd, shutdown = TRUE)\n\n\n\n6.1.3 PostgreSQL\nTo access a PostgreSQL database instead, you can do the following, assuming the database has been created and you have a username and password that allow you to access the particular database.\n\nlibrary(RPostgreSQL)\ndrv &lt;- dbDriver(\"PostgreSQL\")\ndbp &lt;- dbConnect(drv, dbname = 'stackoverflow', user = 'paciorek', password = 'test')\n# simple query to get 5 rows from a table, same as with SQLite:\ndbGetQuery(dbp, \"select * from questions limit 5\")  \n\n\n\n\n6.2 Using SQL from Python\n\n6.2.1 SQLite\n\nimport sqlite3 as sq\ndir = 'data' # relative or absolute path to where the .db file is\ndbFilename = 'stackoverflow-2021.db'\nimport os\ndb = sq.connect(os.path.join('data', dbFilename))\nc = db.cursor()\nc.execute(\"select * from questions limit 5\")  # simple query \nresults = c.fetchall() # retrieve results\n\nTo disconnect:\n\nc.close()\n\n\n\n6.2.2 DuckDB\n\nimport duckdb\ndir = 'data' # relative or absolute path to where the .duckdb file is\ndbFilename = 'stackoverflow-2021.duckdb'\nimport os\ndb = duckdb.connect(os.path.join(dir, dbFilename))\ndb.sql(\"select * from questions limit 5\")\n\n\n\n6.2.3 PostgreSQL\n\nimport psycopg2 as pg\ndb = pg.connect(\"dbname = 'stackoverflow' user = 'paciorek' host = 'localhost' password = 'test'\")\nc = db.cursor()",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#references-and-other-resources",
    "href": "index.html#references-and-other-resources",
    "title": "Working with large datasets using SQL, R, and Python",
    "section": "7 References and Other Resources",
    "text": "7 References and Other Resources\nIn addition to various material found online, including various software manuals and vignettes, much of the SQL material was based on the following two sources:\n\nThe Stanford online Introduction to Databases course (MOOC) released in Fall 2011, a version of which is available on edX.\nHarrison Dekker’s materials from a Statistics short course he taught in January 2016.\n\nI’ve heard good things about the interactive exercises/tutorials at SQLZoo and the book Practical SQL by Anthony DeBarros (available through Berkeley’s library); in particular the first 200 or so pages (through chapter 12) cover general SQL programming/querying.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  }
]